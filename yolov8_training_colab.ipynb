{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# ğŸ› Google Colabä¸Šã§ã®YOLOv8æ˜†è™«æ¤œå‡ºãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n\n**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: æ˜†è™«æ¤œå‡ºãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ  \n**ç›®çš„**: Roboflowãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ–ãƒˆãƒ ã‚·æ¤œå‡ºã®ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´  \n**ç’°å¢ƒ**: GPUåŠ é€Ÿä»˜ãGoogle Colaboratory  \n\n---\n\n## ğŸ“‹ æ¦‚è¦\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯YOLOv8æ˜†è™«æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚å«ã¾ã‚Œã‚‹æ©Ÿèƒ½:\n\n- âœ… è‡ªå‹•åŒ–ã•ã‚ŒãŸç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n- âœ… GPUè¨­å®šã¨æ¤œè¨¼\n- âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨æ¤œè¨¼\n- âœ… ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n- âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–\n- âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n- âœ… çµæœã®å¯è¦–åŒ–\n\n---\n\n## âš¡ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ\n\n1. **GPUã‚’æœ‰åŠ¹åŒ–**: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¿ã‚¤ãƒ—ã®å¤‰æ›´ â†’ GPUã‚’é¸æŠ\n2. **ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ**: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ\n3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n4. **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç›£è¦–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€²æ—ã‚’ç›£è¦–\n5. **çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Google Driveã«ä¿å­˜\n\n---",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": "## ğŸ› ï¸ ã‚¹ãƒ†ãƒƒãƒ—1: ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install-libraries"
   },
   "outputs": [],
   "source": "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nprint(\"ğŸ”§ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n\n!pip install ultralytics roboflow supervision\n!pip install --upgrade torch torchvision\n\nprint(\"âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nimport os\nimport sys\nimport time\nimport shutil\nimport zipfile\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\nimport torch\nimport torchvision\nfrom ultralytics import YOLO\n\n# ãƒ‡ãƒ¼ã‚¿æ“ä½œã¨å¯è¦–åŒ–\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image, display, HTML, clear_output\nimport cv2\n\n# Google Colabå°‚ç”¨\nfrom google.colab import files, drive\nimport yaml\n\nprint(\"ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\nprint(f\"ğŸ Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}\")\nprint(f\"ğŸ”¥ PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}\")\nprint(f\"ğŸ‘ï¸ OpenCVãƒãƒ¼ã‚¸ãƒ§ãƒ³: {cv2.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-header"
   },
   "source": "## ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—2: GPUè¨­å®šã¨ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": "# GPUå¯ç”¨æ€§ã¨è¨­å®šã®ç¢ºèª\ndef check_gpu_setup():\n    \"\"\"GPUè¨­å®šã‚’ãƒã‚§ãƒƒã‚¯ã—ã€åˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®šã™ã‚‹\"\"\"\n    print(\"ğŸ” GPUè¨­å®šã‚’ç¢ºèªä¸­...\")\n    print(\"=\"*50)\n    \n    # CUDAå¯ç”¨æ€§ã®ç¢ºèª\n    cuda_available = torch.cuda.is_available()\n    print(f\"CUDAåˆ©ç”¨å¯èƒ½: {cuda_available}\")\n    \n    if cuda_available:\n        device_count = torch.cuda.device_count()\n        print(f\"GPUæ•°: {device_count}\")\n        \n        for i in range(device_count):\n            gpu_name = torch.cuda.get_device_name(i)\n            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n            print(f\"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n        \n        # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š\n        device = torch.device('cuda:0')\n        print(f\"\\nâœ… ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n        \n        # ç°¡å˜ãªæ¼”ç®—ã§GPUã‚’ãƒ†ã‚¹ãƒˆ\n        test_tensor = torch.rand(1000, 1000).to(device)\n        result = torch.mm(test_tensor, test_tensor.t())\n        print(\"âœ… GPUãƒ†ã‚¹ãƒˆæ“ä½œãŒæˆåŠŸã—ã¾ã—ãŸï¼\")\n        \n    else:\n        print(\"âš ï¸ GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯CPUã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆä½é€Ÿï¼‰ã€‚\")\n        device = torch.device('cpu')\n    \n    print(\"=\"*50)\n    return device\n\n# GPUç¢ºèªã®å®Ÿè¡Œ\ntraining_device = check_gpu_setup()"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "system-info"
   },
   "outputs": [],
   "source": "# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º\ndef display_system_info():\n    \"\"\"ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹\"\"\"\n    print(\"ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±\")\n    print(\"=\"*40)\n    \n    # CPUæƒ…å ±\n    print(f\"CPUã‚³ã‚¢æ•°: {os.cpu_count()}\")\n    \n    # ãƒ¡ãƒ¢ãƒªæƒ…å ±ï¼ˆæ¦‚ç®—ï¼‰\n    import psutil\n    memory = psutil.virtual_memory()\n    print(f\"RAM: {memory.total / 1e9:.1f} GB ï¼ˆåˆ©ç”¨å¯èƒ½: {memory.available / 1e9:.1f} GBï¼‰\")\n    \n    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡\n    disk = psutil.disk_usage('/')\n    print(f\"ãƒ‡ã‚£ã‚¹ã‚¯: {disk.total / 1e9:.1f} GB ï¼ˆç©ºã: {disk.free / 1e9:.1f} GBï¼‰\")\n    \n    print(\"\\nğŸ”§ ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒãƒ¼ã‚¸ãƒ§ãƒ³\")\n    print(\"=\"*40)\n    print(f\"Python: {sys.version.split()[0]}\")\n    print(f\"PyTorch: {torch.__version__}\")\n    print(f\"Torchvision: {torchvision.__version__}\")\n    print(f\"OpenCV: {cv2.__version__}\")\n    print(f\"NumPy: {np.__version__}\")\n    \ndisplay_system_info()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drive-header"
   },
   "source": "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—3: Google Driveé€£æºã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\nprint(\"ğŸ“ Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...\")\ndrive.mount('/content/drive')\n\n# Google Driveå†…ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\nproject_dir = Path('/content/drive/MyDrive/insect_detection_training')\nproject_dir.mkdir(exist_ok=True)\n\n# ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n(project_dir / 'datasets').mkdir(exist_ok=True)\n(project_dir / 'models').mkdir(exist_ok=True)\n(project_dir / 'results').mkdir(exist_ok=True)\n(project_dir / 'logs').mkdir(exist_ok=True)\n\nprint(f\"âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {project_dir}\")\nprint(f\"ğŸ“‚ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n\n# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\nos.chdir('/content')\nprint(f\"ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´ã—ã¾ã—ãŸ: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset-header"
   },
   "source": "## ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n\n### ã‚ªãƒ—ã‚·ãƒ§ãƒ³A: ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "upload-dataset"
   },
   "outputs": [],
   "source": "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³A: ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\ndef upload_dataset_local():\n    \"\"\"ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\"\"\"\n    print(\"ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n    print(\"ZIPå†…ã®æœŸå¾…ã•ã‚Œã‚‹æ§‹é€ :\")\n    print(\"\"\"\n    dataset.zip\n    â”œâ”€â”€ train/\n    â”‚   â”œâ”€â”€ images/\n    â”‚   â””â”€â”€ labels/\n    â”œâ”€â”€ valid/\n    â”‚   â”œâ”€â”€ images/\n    â”‚   â””â”€â”€ labels/\n    â”œâ”€â”€ test/\n    â”‚   â”œâ”€â”€ images/\n    â”‚   â””â”€â”€ labels/\n    â””â”€â”€ data.yaml\n    \"\"\")\n    \n    uploaded = files.upload()\n    \n    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹\n    for filename in uploaded.keys():\n        if filename.endswith('.zip'):\n            print(f\"ğŸ“¦ {filename}ã‚’å±•é–‹ä¸­...\")\n            with zipfile.ZipFile(filename, 'r') as zip_ref:\n                zip_ref.extractall('datasets')\n            print(\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å±•é–‹ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n            break\n    else:\n        print(\"âŒ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å«ã‚€ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n        return False\n    \n    return True\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\nupload_success = upload_dataset_local()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roboflow-header"
   },
   "source": "### ã‚ªãƒ—ã‚·ãƒ§ãƒ³B: Roboflowã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "roboflow-download"
   },
   "outputs": [],
   "source": "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³B: Roboflowã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\ndef download_roboflow_dataset():\n    \"\"\"Roboflowã‹ã‚‰ã‚«ãƒ–ãƒˆãƒ ã‚·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\"\"\"\n    print(\"ğŸ¤– Roboflowã‹ã‚‰ã‚«ãƒ–ãƒˆãƒ ã‚·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n    \n    try:\n        from roboflow import Roboflow\n        \n        # Roboflowã®åˆæœŸåŒ–ï¼ˆAPIã‚­ãƒ¼ã®è¨­å®šãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ï¼‰\n        # APIã‚­ãƒ¼ã®å–å¾—å…ˆ: https://app.roboflow.com/settings/api\n        print(\"ğŸ”‘ Roboflow APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã¯Enterã‚’æŠ¼ã—ã¦ãã ã•ã„ï¼‰:\")\n        api_key = input(\"APIã‚­ãƒ¼: \").strip()\n        \n        if api_key:\n            rf = Roboflow(api_key=api_key)\n            project = rf.workspace(\"z-algae-bilby\").project(\"beetle\")\n            dataset = project.version(1).download(\"yolov8\", location=\"datasets\")\n            print(\"âœ… Roboflowã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼\")\n            return True\n        else:\n            print(\"âš ï¸ APIã‚­ãƒ¼ãŒæä¾›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã‹ã‚‰æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™:\")\n            print(\"https://universe.roboflow.com/z-algae-bilby/beetle/dataset/1\")\n            return False\n            \n    except Exception as e:\n        print(f\"âŒ Roboflowã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}\")\n        print(\"ğŸ’¡ ä»£æ›¿æ¡ˆ: æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³Aã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n        return False\n\n# Roboflowã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\ndownload_success = download_roboflow_dataset()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "manual-setup-header"
   },
   "source": "### ã‚ªãƒ—ã‚·ãƒ§ãƒ³C: æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "manual-dataset"
   },
   "outputs": [],
   "source": "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³C: ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\ndef create_sample_dataset():\n    \"\"\"ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã‚’ä½œæˆã™ã‚‹\"\"\"\n    print(\"ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã‚’ä½œæˆä¸­...\")\n    \n    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä½œæˆ\n    base_dir = Path('datasets')\n    for split in ['train', 'valid', 'test']:\n        (base_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n        (base_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n    \n    # ã‚µãƒ³ãƒ—ãƒ«data.yamlã®ä½œæˆ\n    data_yaml = {\n        'train': './train/images',\n        'val': './valid/images', \n        'test': './test/images',\n        'nc': 1,\n        'names': ['beetle'],\n        'roboflow': {\n            'workspace': 'z-algae-bilby',\n            'project': 'beetle',\n            'version': 1,\n            'license': 'CC BY 4.0',\n            'url': 'https://universe.roboflow.com/z-algae-bilby/beetle/dataset/1'\n        }\n    }\n    \n    with open(base_dir / 'data.yaml', 'w') as f:\n        yaml.dump(data_yaml, f, default_flow_style=False)\n    \n    print(\"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã‚’ä½œæˆã—ã¾ã—ãŸï¼\")\n    print(\"âš ï¸ æ³¨æ„: ã“ã‚Œã¯æ§‹é€ ã®ã¿ã§ã™ã€‚å®Ÿéš›ã®ç”»åƒã¨ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n    return True\n\n# ã‚µãƒ³ãƒ—ãƒ«æ§‹é€ ã‚’ä½œæˆã™ã‚‹ã«ã¯ä»¥ä¸‹ã®è¡Œã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„\n# create_sample_dataset()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation-header"
   },
   "source": "## âœ… ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼ã¨è§£æ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "validate-dataset"
   },
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã¨å†…å®¹ã®æ¤œè¨¼\ndef validate_dataset(dataset_path='datasets'):\n    \"\"\"\n    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã¨å†…å®¹ã‚’æ¤œè¨¼ã™ã‚‹é–¢æ•°\n    \n    Args:\n        dataset_path: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹\n    \n    Returns:\n        validation_success: æ¤œè¨¼ãŒæˆåŠŸã—ãŸã‹ã®ãƒ–ãƒ¼ãƒ«å€¤\n        dataset_config: data.yamlã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¨­å®šæƒ…å ±\n        dataset_stats: å„åˆ†å‰²ï¼ˆtrain/valid/testï¼‰ã®çµ±è¨ˆæƒ…å ±\n    \"\"\"\n    print(\"ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã‚’æ¤œè¨¼ä¸­...\")\n    print(\"=\"*50)\n    \n    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®Pathã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n    dataset_dir = Path(dataset_path)\n    \n    # data.yamlãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªï¼ˆYOLOå½¢å¼ã§ã¯å¿…é ˆï¼‰\n    data_yaml_path = dataset_dir / 'data.yaml'\n    if not data_yaml_path.exists():\n        print(\"âŒ data.yaml not found!\")\n        return False\n    \n    # YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—\n    with open(data_yaml_path, 'r') as f:\n        data_config = yaml.safe_load(f)\n    \n    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šæƒ…å ±ã®è¡¨ç¤ºï¼ˆroboflowæƒ…å ±ã¯é™¤ãï¼‰\n    print(\"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š (data.yaml):\")\n    for key, value in data_config.items():\n        if key != 'roboflow':  # Roboflowå›ºæœ‰ã®æƒ…å ±ã¯è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—\n            print(f\"  {key}: {value}\")\n    \n    # å„åˆ†å‰²ï¼ˆtrain/valid/testï¼‰ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã¨ç”»åƒãƒ»ãƒ©ãƒ™ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯\n    results = {}\n    for split in ['train', 'valid', 'test']:\n        # ç”»åƒã¨ãƒ©ãƒ™ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’æ§‹ç¯‰\n        images_dir = dataset_dir / split / 'images'\n        labels_dir = dataset_dir / split / 'labels'\n        \n        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n        if images_dir.exists() and labels_dir.exists():\n            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆ.jpg, .png, .jpegæ‹¡å¼µå­ï¼‰\n            image_files = list(images_dir.glob('*.[jp][pn]g')) + list(images_dir.glob('*.jpeg'))\n            # ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆ.txtæ‹¡å¼µå­ã€YOLOå½¢å¼ï¼‰\n            label_files = list(labels_dir.glob('*.txt'))\n            \n            # çµ±è¨ˆæƒ…å ±ã‚’è¾æ›¸ã«ä¿å­˜\n            results[split] = {\n                'images': len(image_files),\n                'labels': len(label_files)\n            }\n            \n            # ç”»åƒæ•°ã¨ãƒ©ãƒ™ãƒ«æ•°ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n            # ä¸€è‡´ã—ã¦ã„ã¦ã€ã‹ã¤0ã‚ˆã‚Šå¤§ãã„å ´åˆã¯æ­£å¸¸\n            status = \"âœ…\" if len(image_files) == len(label_files) and len(image_files) > 0 else \"âš ï¸\"\n            print(f\"{status} {split.upper()}: {len(image_files)} ç”»åƒ, {len(label_files)} ãƒ©ãƒ™ãƒ«\")\n        else:\n            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ\n            print(f\"âŒ {split.upper()}: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n            results[split] = {'images': 0, 'labels': 0}\n    \n    # å…¨ä½“ã®çµ±è¨ˆã‚’è¨ˆç®—\n    total_images = sum(split['images'] for split in results.values())\n    total_labels = sum(split['labels'] for split in results.values())\n    \n    print(f\"\\nğŸ“Š åˆè¨ˆ: {total_images} ç”»åƒ, {total_labels} ãƒ©ãƒ™ãƒ«\")\n    \n    # æ¤œè¨¼æˆåŠŸã®æ¡ä»¶ï¼šç”»åƒãŒå­˜åœ¨ã—ã€ç”»åƒæ•°ã¨ãƒ©ãƒ™ãƒ«æ•°ãŒä¸€è‡´\n    if total_images > 0 and total_images == total_labels:\n        print(\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼ãŒæˆåŠŸã—ã¾ã—ãŸï¼\")\n        return True, data_config, results\n    else:\n        print(\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼ãŒå¤±æ•—ã—ã¾ã—ãŸï¼\")\n        return False, None, None\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼ã‚’å®Ÿè¡Œ\nvalidation_success, dataset_config, dataset_stats = validate_dataset()"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "visualize-dataset"
   },
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆã®å¯è¦–åŒ–\ndef visualize_dataset_stats(stats):\n    \"\"\"\n    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆæƒ…å ±ã‚’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã™ã‚‹é–¢æ•°\n    \n    Args:\n        stats: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆæƒ…å ±ã®è¾æ›¸\n               å½¢å¼: {'train': {'images': æ•°å€¤, 'labels': æ•°å€¤}, ...}\n    \"\"\"\n    if not stats:\n        print(\"âŒ è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆãŒã‚ã‚Šã¾ã›ã‚“\")\n        return\n    \n    print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆã®å¯è¦–åŒ–\")\n    \n    # matplotlibå›³ã‚’ä½œæˆï¼ˆ1è¡Œ2åˆ—ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆï¼‰\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # ãƒ—ãƒ­ãƒƒãƒˆ1: åˆ†å‰²ã”ã¨ã®ç”»åƒæ•°ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º\n    splits = list(stats.keys())  # ['train', 'valid', 'test']\n    image_counts = [stats[split]['images'] for split in splits]  # å„åˆ†å‰²ã®ç”»åƒæ•°\n    \n    # ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆã‚’è¨­å®šï¼ˆè¦–è¦šçš„ã«åŒºåˆ¥ã—ã‚„ã™ã„è‰²ï¼‰\n    bars1 = ax1.bar(splits, image_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n    ax1.set_title('åˆ†å‰²ã”ã¨ã®ç”»åƒæ•°', fontsize=14, fontweight='bold')\n    ax1.set_ylabel('ç”»åƒæ•°')\n    \n    # æ£’ã‚°ãƒ©ãƒ•ã®ä¸Šã«æ•°å€¤ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ \n    for bar, count in zip(bars1, image_counts):\n        # æ£’ã®ä¸­å¤®ä¸Šéƒ¨ã«æ•°å€¤ã‚’è¡¨ç¤º\n        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n                str(count), ha='center', va='bottom', fontweight='bold')\n    \n    # ãƒ—ãƒ­ãƒƒãƒˆ2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã®å‰²åˆã‚’å††ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º\n    total = sum(image_counts)  # å…¨ç”»åƒæ•°\n    percentages = [count/total*100 for count in image_counts]  # å„åˆ†å‰²ã®å‰²åˆ\n    \n    # å††ã‚°ãƒ©ãƒ•ã‚’ä½œæˆï¼ˆåˆ†å‰²åã¨ç”»åƒæ•°ã€å‰²åˆã‚’è¡¨ç¤ºï¼‰\n    ax2.pie(percentages, labels=[f'{split}\\n({count} ç”»åƒ)' for split, count in zip(splits, image_counts)], \n            autopct='%1.1f%%', startangle=90, colors=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n    ax2.set_title('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã®å‰²åˆ', fontsize=14, fontweight='bold')\n    \n    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’èª¿æ•´ã—ã¦è¡¨ç¤º\n    plt.tight_layout()\n    plt.show()\n    \n    # ãƒ†ã‚­ã‚¹ãƒˆã§ã®ã‚µãƒãƒªãƒ¼ã‚‚è¡¨ç¤º\n    print(f\"\\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µãƒãƒªãƒ¼:\")\n    print(f\"ç·ç”»åƒæ•°: {total}\")\n    for split, count in zip(splits, image_counts):\n        percentage = count/total*100\n        print(f\"{split.upper()}: {count} ç”»åƒ ({percentage:.1f}%)\")\n\n# æ¤œè¨¼ãŒæˆåŠŸã—ãŸå ´åˆã®ã¿å¯è¦–åŒ–ã‚’å®Ÿè¡Œ\nif validation_success:\n    visualize_dataset_stats(dataset_stats)\nelse:\n    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å¯è¦–åŒ–ã§ãã¾ã›ã‚“ - æ¤œè¨¼ãŒå¤±æ•—ã—ã¾ã—ãŸ\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sample-images"
   },
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®è¡¨ç¤º\ndef display_sample_images(dataset_path='datasets', num_samples=6):\n    \"\"\"\n    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’é¸æŠã—ã¦è¡¨ç¤ºã™ã‚‹é–¢æ•°\n    \n    Args:\n        dataset_path: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹\n        num_samples: è¡¨ç¤ºã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ç”»åƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š6æšï¼‰\n    \"\"\"\n    if not validation_success:\n        print(\"âš ï¸ ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼ãŒå¤±æ•—ã—ã¾ã—ãŸ\")\n        return\n    \n    print(f\"ğŸ–¼ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‹ã‚‰{num_samples}æšã®ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’è¡¨ç¤º\")\n    \n    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰\n    dataset_dir = Path(dataset_path)\n    train_images = list((dataset_dir / 'train' / 'images').glob('*.[jp][pn]g'))\n    \n    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n    if len(train_images) == 0:\n        print(\"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã«ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n        return\n    \n    # åˆ©ç”¨å¯èƒ½ãªç”»åƒæ•°ã¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®å°ã•ã„æ–¹ã‚’é¸æŠ\n    # ãƒ©ãƒ³ãƒ€ãƒ ã«ç”»åƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆé‡è¤‡ãªã—ï¼‰\n    sample_images = np.random.choice(train_images, min(num_samples, len(train_images)), replace=False)\n    \n    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®é…ç½®ã‚’è¨ˆç®—ï¼ˆ3åˆ—å›ºå®šï¼‰\n    cols = 3\n    rows = (num_samples + cols - 1) // cols  # å¿…è¦ãªè¡Œæ•°ã‚’è¨ˆç®—\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n    \n    # 1è¡Œã®å ´åˆã¯2æ¬¡å…ƒé…åˆ—ã«å¤‰æ›ï¼ˆçµ±ä¸€çš„ã«æ‰±ã†ãŸã‚ï¼‰\n    if rows == 1:\n        axes = axes.reshape(1, -1)\n    \n    # å„ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’å‡¦ç†ã—ã¦è¡¨ç¤º\n    for i, img_path in enumerate(sample_images):\n        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®ä½ç½®ã‚’è¨ˆç®—\n        row = i // cols\n        col = i % cols\n        \n        # OpenCVã§ç”»åƒã‚’èª­ã¿è¾¼ã¿ï¼ˆBGRå½¢å¼ï¼‰\n        img = cv2.imread(str(img_path))\n        # matplotlibã§è¡¨ç¤ºã™ã‚‹ãŸã‚ã«RGBå½¢å¼ã«å¤‰æ›\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # ç”»åƒã‚’è¡¨ç¤º\n        axes[row, col].imshow(img_rgb)\n        axes[row, col].set_title(f\"ã‚µãƒ³ãƒ—ãƒ« {i+1}: {img_path.name}\", fontsize=10)\n        axes[row, col].axis('off')  # è»¸ã‚’éè¡¨ç¤º\n    \n    # ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤ºã«ã™ã‚‹\n    for i in range(len(sample_images), rows * cols):\n        row = i // cols\n        col = i % cols\n        axes[row, col].axis('off')\n    \n    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’èª¿æ•´ã—ã¦è¡¨ç¤º\n    plt.tight_layout()\n    plt.show()\n\n# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®è¡¨ç¤ºã‚’å®Ÿè¡Œ\ndisplay_sample_images()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-header"
   },
   "source": "## ğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—6: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«é¸æŠ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚¯ãƒ©ã‚¹\nclass TrainingConfig:\n    \"\"\"\n    YOLOv8ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªè¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹\n    \n    ã“ã®ã‚¯ãƒ©ã‚¹ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€å…ƒç®¡ç†ã—ã€\n    GPUç’°å¢ƒã«å¿œã˜ãŸè‡ªå‹•æœ€é©åŒ–ã‚‚è¡Œã„ã¾ã™ã€‚\n    \"\"\"\n    \n    def __init__(self):\n        # === ãƒ¢ãƒ‡ãƒ«è¨­å®š ===\n        # YOLOv8ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆn=nano, s=small, m=medium, l=large, x=extra-largeï¼‰\n        # å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã»ã©é«˜é€Ÿã ãŒç²¾åº¦ã¯ä½ã„ã€å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã»ã©é«˜ç²¾åº¦ã ãŒè¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„\n        self.model_size = 'n'  \n        self.pretrained_model = f'yolov8{self.model_size}.pt'  # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å\n        \n        # === ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===\n        self.epochs = 100          # è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’ä½•å›å­¦ç¿’ã™ã‚‹ã‹ï¼‰\n        self.batch_size = 16       # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆä¸€åº¦ã«å‡¦ç†ã™ã‚‹ç”»åƒæ•°ã€GPU ãƒ¡ãƒ¢ãƒªã«ä¾å­˜ï¼‰\n        self.image_size = 640      # å…¥åŠ›ç”»åƒã‚µã‚¤ã‚ºï¼ˆYOLOæ¨™æº–ã¯640x640ãƒ”ã‚¯ã‚»ãƒ«ï¼‰\n        self.device = 'auto'       # ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆ'auto'=è‡ªå‹•é¸æŠ, 'cpu', '0'=GPU0ãªã©ï¼‰\n        \n        # === ãƒ‡ãƒ¼ã‚¿è¨­å®š ===\n        self.data_yaml = 'datasets/data.yaml'  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n        \n        # === å‡ºåŠ›è¨­å®š ===\n        self.project_name = 'training_results'        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåï¼ˆçµæœä¿å­˜ç”¨ï¼‰\n        self.experiment_name = 'beetle_detection_colab' # å®Ÿé¨“åï¼ˆã“ã®å®Ÿè¡Œã®è­˜åˆ¥å­ï¼‰\n        \n        # === é«˜åº¦ãªè¨­å®š ===\n        self.patience = 50      # æ—©æœŸåœæ­¢ã®å¿è€å€¤ï¼ˆä½•ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãŒãªã‘ã‚Œã°åœæ­¢ã™ã‚‹ã‹ï¼‰\n        self.save_period = 10   # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”ï¼ˆNã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ä¸­é–“çµæœã‚’ä¿å­˜ï¼‰\n        self.workers = 2        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆä¸¦åˆ—å‡¦ç†æ•°ï¼‰\n        \n        # === æœ€é©åŒ–è¨­å®š ===\n        self.optimizer = 'AdamW'    # æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆSGD, Adam, AdamWã‹ã‚‰é¸æŠï¼‰\n        self.lr0 = 0.01            # åˆæœŸå­¦ç¿’ç‡ï¼ˆå­¦ç¿’ã®é€Ÿåº¦ã‚’åˆ¶å¾¡ï¼‰\n        self.weight_decay = 0.0005  # é‡ã¿æ¸›è¡°ï¼ˆéå­¦ç¿’ã‚’é˜²ãæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n        \n    def display_config(self):\n        \"\"\"ç¾åœ¨ã®è¨­å®šã‚’è¦‹ã‚„ã™ã„å½¢å¼ã§è¡¨ç¤ºã™ã‚‹\"\"\"\n        print(\"ğŸ¯ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\")\n        print(\"=\"*40)\n        print(f\"ãƒ¢ãƒ‡ãƒ«: {self.pretrained_model}\")\n        print(f\"ã‚¨ãƒãƒƒã‚¯æ•°: {self.epochs}\")\n        print(f\"ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}\")\n        print(f\"ç”»åƒã‚µã‚¤ã‚º: {self.image_size}\")\n        print(f\"ãƒ‡ãƒã‚¤ã‚¹: {self.device}\")\n        print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {self.data_yaml}\")\n        print(f\"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {self.project_name}/{self.experiment_name}\")\n        print(f\"æœ€é©åŒ–æ‰‹æ³•: {self.optimizer}\")\n        print(f\"å­¦ç¿’ç‡: {self.lr0}\")\n        print(f\"é‡ã¿æ¸›è¡°: {self.weight_decay}\")\n        print(\"=\"*40)\n\n# è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\nconfig = TrainingConfig()\nconfig.display_config()\n\n# GPU ãƒ¡ãƒ¢ãƒªã«åŸºã¥ããƒãƒƒãƒã‚µã‚¤ã‚ºã®è‡ªå‹•èª¿æ•´\nif torch.cuda.is_available():\n    # GPU ãƒ¡ãƒ¢ãƒªå®¹é‡ã‚’å–å¾—ï¼ˆGBå˜ä½ï¼‰\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"\\nğŸ® GPU ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f} GB\")\n    \n    # ãƒ¡ãƒ¢ãƒªå®¹é‡ã«å¿œã˜ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´\n    if gpu_memory < 8:\n        # 8GBæœªæº€ï¼šãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’é¿ã‘ã‚‹ãŸã‚å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚º\n        config.batch_size = 8\n        print(\"âš¡ GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’8ã«å‰Šæ¸›ã—ã¾ã—ãŸ\")\n    elif gpu_memory >= 16:\n        # 16GBä»¥ä¸Šï¼šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã§GPUã‚’åŠ¹ç‡æ´»ç”¨\n        config.batch_size = 32\n        print(\"ğŸš€ GPUåˆ©ç”¨åŠ¹ç‡å‘ä¸Šã®ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’32ã«å¢—åŠ ã—ã¾ã—ãŸ\")\n    \nprint(f\"\\nğŸ“Š æœ€çµ‚ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-training-header"
   },
   "source": "## ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": "# äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\ndef load_pretrained_model(model_name):\n    \"\"\"\n    æŒ‡å®šã•ã‚ŒãŸYOLOv8äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°\n    \n    Args:\n        model_name: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹ï¼š'yolov8n.pt'ï¼‰\n    \n    Returns:\n        model: èª­ã¿è¾¼ã¾ã‚ŒãŸYOLOãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆæˆåŠŸæ™‚ï¼‰ã¾ãŸã¯Noneï¼ˆå¤±æ•—æ™‚ï¼‰\n    \"\"\"\n    print(f\"ğŸ“¥ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_name}\")\n    \n    try:\n        # Ultralyticsã®YOLOã‚¯ãƒ©ã‚¹ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n        # åˆå›å®Ÿè¡Œæ™‚ã¯è‡ªå‹•çš„ã«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹\n        model = YOLO(model_name)\n        print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒæˆåŠŸã—ã¾ã—ãŸï¼\")\n        \n        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º\n        print(f\"\\nğŸ“‹ ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n        print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_name}\")\n        print(f\"ã‚¿ã‚¹ã‚¯: {model.task}\")  # 'detect'ï¼ˆç‰©ä½“æ¤œå‡ºï¼‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹\n        \n        return model\n    \n    except Exception as e:\n        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†\n        print(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n        return None\n\n# è¨­å®šã§æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\nmodel = load_pretrained_model(config.pretrained_model)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training-execution"
   },
   "outputs": [],
   "source": "# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã®å®Ÿè¡Œ\ndef train_model(model, config):\n    \"\"\"\n    YOLOv8ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n    \n    Args:\n        model: èª­ã¿è¾¼ã¿æ¸ˆã¿ã®YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n        config: TrainingConfigã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆè¨“ç·´è¨­å®šï¼‰\n    \n    Returns:\n        results: è¨“ç·´çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆæˆåŠŸæ™‚ï¼‰ã¾ãŸã¯Noneï¼ˆå¤±æ•—æ™‚ï¼‰\n    \"\"\"\n    # å‰ææ¡ä»¶ã®ãƒã‚§ãƒƒã‚¯\n    if model is None:\n        print(\"âŒ è¨“ç·´ã‚’é–‹å§‹ã§ãã¾ã›ã‚“ - ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n        return None\n    \n    if not validation_success:\n        print(\"âŒ è¨“ç·´ã‚’é–‹å§‹ã§ãã¾ã›ã‚“ - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼ãŒå¤±æ•—ã—ã¾ã—ãŸ\")\n        return None\n    \n    # è¨“ç·´é–‹å§‹ã®é€šçŸ¥\n    print(\"ğŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...\")\n    print(\"â±ï¸ è¨­å®šã«ã‚ˆã£ã¦ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™\")\n    print(\"ğŸ“Š è¨“ç·´ã®é€²æ—ã¯ä»¥ä¸‹ã«è¡¨ç¤ºã•ã‚Œã¾ã™\")\n    print(\"=\"*60)\n    \n    # è¨“ç·´é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²ï¼ˆè¨“ç·´æ™‚é–“è¨ˆæ¸¬ã®ãŸã‚ï¼‰\n    start_time = time.time()\n    \n    try:\n        # YOLOv8ã®trainãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦è¨“ç·´ã‚’å®Ÿè¡Œ\n        # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜ï¼š\n        # - data: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆdata.yamlï¼‰ã®ãƒ‘ã‚¹\n        # - epochs: è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’ä½•å›å­¦ç¿’ã™ã‚‹ã‹ï¼‰\n        # - batch: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆä¸€åº¦ã«å‡¦ç†ã™ã‚‹ç”»åƒæ•°ï¼‰\n        # - imgsz: å…¥åŠ›ç”»åƒã‚µã‚¤ã‚ºï¼ˆãƒªã‚µã‚¤ã‚ºå¾Œã®ã‚µã‚¤ã‚ºï¼‰\n        # - device: ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU/CPUï¼‰\n        # - project: çµæœä¿å­˜ç”¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå\n        # - name: ã“ã®å®Ÿé¨“ã®åå‰ï¼ˆãƒ•ã‚©ãƒ«ãƒ€åã¨ã—ã¦ä½¿ç”¨ï¼‰\n        # - save: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹\n        # - save_period: ä¸­é–“çµæœã®ä¿å­˜é–“éš”\n        # - patience: æ—©æœŸåœæ­¢ã®å¿è€å€¤ï¼ˆæ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªã„ã‚¨ãƒãƒƒã‚¯æ•°ï¼‰\n        # - workers: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä¸¦åˆ—å‡¦ç†æ•°\n        # - optimizer: æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \n        # - lr0: åˆæœŸå­¦ç¿’ç‡\n        # - weight_decay: é‡ã¿æ¸›è¡°ï¼ˆæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n        # - val: æ¤œè¨¼ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹\n        # - plots: çµæœã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ã‹ã©ã†ã‹\n        # - verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹\n        results = model.train(\n            data=config.data_yaml,\n            epochs=config.epochs,\n            batch=config.batch_size,\n            imgsz=config.image_size,\n            device=config.device,\n            project=config.project_name,\n            name=config.experiment_name,\n            save=True,\n            save_period=config.save_period,\n            patience=config.patience,\n            workers=config.workers,\n            optimizer=config.optimizer,\n            lr0=config.lr0,\n            weight_decay=config.weight_decay,\n            val=True,\n            plots=True,\n            verbose=True\n        )\n        \n        # è¨“ç·´æ™‚é–“ã®è¨ˆç®—ã¨è¡¨ç¤º\n        training_time = time.time() - start_time\n        hours = int(training_time // 3600)      # æ™‚é–“\n        minutes = int((training_time % 3600) // 60)  # åˆ†\n        seconds = int(training_time % 60)       # ç§’\n        \n        # è¨“ç·´å®Œäº†ã®é€šçŸ¥\n        print(\"=\"*60)\n        print(f\"âœ… è¨“ç·´ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼\")\n        print(f\"â±ï¸ ç·è¨“ç·´æ™‚é–“: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n        print(f\"ğŸ“ çµæœä¿å­˜å…ˆ: {config.project_name}/{config.experiment_name}\")\n        \n        return results\n        \n    except Exception as e:\n        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†\n        print(f\"âŒ è¨“ç·´ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n        return None\n\n# è¨“ç·´é–‹å§‹ã®è­¦å‘Šï¼ˆèª¤å®Ÿè¡Œé˜²æ­¢ï¼‰\nprint(\"âš ï¸ è­¦å‘Š: 5ç§’å¾Œã«è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...\")\ntime.sleep(5)\n\n# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚’å®Ÿè¡Œï¼ˆã“ã‚Œã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ï¼‰\ntraining_results = train_model(model, config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-header"
   },
   "source": "## ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—8: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®è§£æã¨å¯è¦–åŒ–",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display-results"
   },
   "outputs": [],
   "source": [
    "# Display training results\n",
    "def display_training_results(results, config):\n",
    "    if results is None:\n",
    "        print(\"âŒ No training results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“Š Training Results Summary\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Results directory\n",
    "    results_dir = Path(config.project_name) / config.experiment_name\n",
    "    \n",
    "    # Display key metrics if available\n",
    "    if hasattr(results, 'results_dict'):\n",
    "        metrics = results.results_dict\n",
    "        print(\"ğŸ¯ Final Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Check for results plots\n",
    "    plots_to_show = [\n",
    "        ('results.png', 'ğŸ“ˆ Training/Validation Curves'),\n",
    "        ('confusion_matrix.png', 'ğŸ¯ Confusion Matrix'),\n",
    "        ('labels.jpg', 'ğŸ“Š Label Distribution'),\n",
    "        ('val_batch0_pred.jpg', 'ğŸ” Validation Predictions')\n",
    "    ]\n",
    "    \n",
    "    for plot_file, title in plots_to_show:\n",
    "        plot_path = results_dir / plot_file\n",
    "        if plot_path.exists():\n",
    "            print(f\"\\n{title}\")\n",
    "            display(Image(str(plot_path)))\n",
    "        else:\n",
    "            print(f\"âš ï¸ {title} not found: {plot_path}\")\n",
    "\n",
    "# Display results\n",
    "if training_results:\n",
    "    display_training_results(training_results, config)\n",
    "else:\n",
    "    print(\"âš ï¸ No training results available to display\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model-validation"
   },
   "outputs": [],
   "source": "# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼å®Ÿè¡Œ\ndef validate_trained_model(config):\n    \"\"\"\n    è¨“ç·´å®Œäº†å¾Œã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æ¤œè¨¼ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n    \n    Args:\n        config: TrainingConfigã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆçµæœã®ä¿å­˜å ´æ‰€ãªã©ã®æƒ…å ±ï¼‰\n    \n    Returns:\n        best_model: èª­ã¿è¾¼ã¾ã‚ŒãŸæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼ˆæˆåŠŸæ™‚ï¼‰ã¾ãŸã¯Noneï¼ˆå¤±æ•—æ™‚ï¼‰\n        val_results: æ¤œè¨¼çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆæˆåŠŸæ™‚ï¼‰ã¾ãŸã¯Noneï¼ˆå¤±æ•—æ™‚ï¼‰\n    \"\"\"\n    print(\"ğŸ§ª æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...\")\n    \n    # è¨“ç·´ã§ç”Ÿæˆã•ã‚ŒãŸæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰\n    best_model_path = Path(config.project_name) / config.experiment_name / 'weights' / 'best.pt'\n    \n    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n    if not best_model_path.exists():\n        print(f\"âŒ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {best_model_path}\")\n        return None, None\n    \n    try:\n        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n        # best.ptã¯è¨“ç·´ä¸­ã«æœ€ã‚‚è‰¯ã„æ€§èƒ½ã‚’ç¤ºã—ãŸã‚¨ãƒãƒƒã‚¯ã®ãƒ¢ãƒ‡ãƒ«\n        best_model = YOLO(str(best_model_path))\n        print(f\"âœ… æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {best_model_path}\")\n        \n        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æ¤œè¨¼ã‚’å®Ÿè¡Œ\n        print(\"\\nğŸ¯ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...\")\n        # val()ãƒ¡ã‚½ãƒƒãƒ‰ã¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æŒ‡æ¨™ã‚’è¨ˆç®—\n        val_results = best_model.val(data=config.data_yaml)\n        \n        # æ¤œè¨¼çµæœã®è¡¨ç¤º\n        if hasattr(val_results, 'box'):\n            box_metrics = val_results.box  # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æ¤œå‡ºã®æŒ‡æ¨™\n            print(\"\\nğŸ“Š æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹:\")\n            # mAP@0.5: IoUé–¾å€¤0.5ã§ã®å¹³å‡ç²¾åº¦ï¼ˆæœ€ã‚‚é‡è¦ãªæŒ‡æ¨™ï¼‰\n            print(f\"  mAP@0.5: {box_metrics.map50:.4f}\")\n            # mAP@0.5:0.95: IoUé–¾å€¤0.5ã€œ0.95ã®å¹³å‡ã§ã®å¹³å‡ç²¾åº¦ï¼ˆã‚ˆã‚Šå³ã—ã„è©•ä¾¡ï¼‰\n            print(f\"  mAP@0.5:0.95: {box_metrics.map:.4f}\")\n            # Precision: æ¤œå‡ºã—ãŸã‚‚ã®ã®ã†ã¡æ­£è§£ã®å‰²åˆ\n            print(f\"  Precision: {box_metrics.mp:.4f}\")\n            # Recall: å®Ÿéš›ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã†ã¡æ¤œå‡ºã§ããŸå‰²åˆ\n            print(f\"  Recall: {box_metrics.mr:.4f}\")\n        \n        return best_model, val_results\n        \n    except Exception as e:\n        print(f\"âŒ æ¤œè¨¼ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n        return None, None\n\n# è¨“ç·´ãŒæˆåŠŸã—ãŸå ´åˆã®ã¿æ¤œè¨¼ã‚’å®Ÿè¡Œ\nif training_results:\n    best_model, validation_results = validate_trained_model(config)\nelse:\n    print(\"âš ï¸ æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ - è¨“ç·´ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“\")\n    best_model, validation_results = None, None"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference-header"
   },
   "source": "## ğŸ” ã‚¹ãƒ†ãƒƒãƒ—9: ãƒ¢ãƒ‡ãƒ«æ¨è«–ã¨ãƒ†ã‚¹ãƒˆ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "test-inference"
   },
   "outputs": [],
   "source": "# ãƒ¢ãƒ‡ãƒ«æ¨è«–ã®ãƒ†ã‚¹ãƒˆ\ndef test_model_inference(model, config, num_samples=4):\n    \"\"\"\n    è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§æ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n    \n    Args:\n        model: è¨“ç·´æ¸ˆã¿ã®YOLOãƒ¢ãƒ‡ãƒ«\n        config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n        num_samples: ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ç”»åƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š4æšï¼‰\n    \"\"\"\n    if model is None:\n        print(\"âŒ ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")\n        return\n    \n    print(f\"ğŸ” {num_samples}æšã®ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’ãƒ†ã‚¹ãƒˆä¸­...\")\n    \n    # ãƒ†ã‚¹ãƒˆç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å–å¾—ï¼ˆtestâ†’validâ†’trainã®é †ã§æ¢ç´¢ï¼‰\n    test_images_dir = Path('datasets/test/images')\n    if not test_images_dir.exists():\n        # ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯æ¤œè¨¼ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨\n        test_images_dir = Path('datasets/valid/images')\n    \n    if not test_images_dir.exists():\n        print(\"âŒ ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n        return\n    \n    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—\n    image_files = list(test_images_dir.glob('*.[jp][pn]g'))\n    if len(image_files) == 0:\n        print(\"âŒ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n        return\n    \n    # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’é¸æŠ\n    sample_images = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n    \n    # 2x2ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    axes = axes.flatten()  # 2æ¬¡å…ƒé…åˆ—ã‚’1æ¬¡å…ƒã«å¤‰æ›\n    \n    for i, img_path in enumerate(sample_images):\n        if i >= len(axes):\n            break\n        \n        try:\n            # ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’å®Ÿè¡Œ\n            # results[0]ã¯æœ€åˆã®ç”»åƒã®çµæœï¼ˆãƒãƒƒãƒå‡¦ç†ã®å ´åˆã¯è¤‡æ•°ï¼‰\n            results = model(str(img_path))\n            \n            # æ¤œå‡ºçµæœã‚’ç”»åƒã«æç”»\n            # plot()ãƒ¡ã‚½ãƒƒãƒ‰ã¯ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨ã‚¯ãƒ©ã‚¹åã‚’æç”»\n            annotated_img = results[0].plot()\n            \n            # OpenCVã®BGRå½¢å¼ã‹ã‚‰matplotlibã®RGBå½¢å¼ã«å¤‰æ›\n            annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n            \n            # ç”»åƒã‚’è¡¨ç¤º\n            axes[i].imshow(annotated_img_rgb)\n            \n            # æ¤œå‡ºæƒ…å ±ã‚’å–å¾—ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã«è¡¨ç¤º\n            detections = len(results[0].boxes) if results[0].boxes is not None else 0\n            confidence = results[0].boxes.conf.max().item() if detections > 0 else 0\n            \n            axes[i].set_title(f\"{img_path.name}\\næ¤œå‡ºæ•°: {detections}, æœ€å¤§ä¿¡é ¼åº¦: {confidence:.3f}\", \n                            fontsize=10)\n            axes[i].axis('off')  # è»¸ã‚’éè¡¨ç¤º\n            \n        except Exception as e:\n            print(f\"âŒ {img_path.name}ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\")\n            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º\n            axes[i].text(0.5, 0.5, f\"ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...\", \n                        ha='center', va='center', transform=axes[i].transAxes)\n            axes[i].axis('off')\n    \n    # ä½¿ç”¨ã—ã¦ã„ãªã„ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º\n    for i in range(len(sample_images), len(axes)):\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.suptitle('ğŸ” ãƒ¢ãƒ‡ãƒ«æ¨è«–çµæœ', fontsize=16, fontweight='bold', y=1.02)\n    plt.show()\n\n# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã«æ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ\nif best_model:\n    test_model_inference(best_model, config)\nelse:\n    print(\"âš ï¸ æ¨è«–ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ - è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-header"
   },
   "source": "## ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—10: ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "export-model"
   },
   "outputs": [],
   "source": "# æ§˜ã€…ãªå½¢å¼ã§ã®ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\ndef export_trained_model(model, config, formats=['onnx', 'torchscript']):\n    \"\"\"\n    è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ§˜ã€…ãªå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹é–¢æ•°\n    \n    Args:\n        model: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹YOLOãƒ¢ãƒ‡ãƒ«\n        config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆç”»åƒã‚µã‚¤ã‚ºãªã©ã®æƒ…å ±ï¼‰\n        formats: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å½¢å¼ã®ãƒªã‚¹ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šONNXã€TorchScriptï¼‰\n    \n    Returns:\n        exported_files: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ\n    \"\"\"\n    if model is None:\n        print(\"âŒ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")\n        return\n    \n    print(f\"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’ä»¥ä¸‹ã®å½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­: {formats}\")\n    \n    exported_files = []\n    \n    for format_type in formats:\n        try:\n            print(f\"\\nğŸ”„ {format_type.upper()}å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...\")\n            \n            # export()ãƒ¡ã‚½ãƒƒãƒ‰ã§æŒ‡å®šå½¢å¼ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n            # format: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼ï¼ˆ'onnx', 'torchscript', 'engine'ãªã©ï¼‰\n            # imgsz: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ™‚ã®ç”»åƒã‚µã‚¤ã‚ºï¼ˆæ¨è«–æ™‚ã¨åŒã˜ã‚µã‚¤ã‚ºã«ã™ã‚‹ï¼‰\n            export_path = model.export(format=format_type, imgsz=config.image_size)\n            exported_files.append(export_path)\n            print(f\"âœ… {format_type.upper()}ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæˆåŠŸ: {export_path}\")\n            \n        except Exception as e:\n            print(f\"âŒ {format_type.upper()}ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¤±æ•—: {e}\")\n    \n    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼\n    if exported_files:\n        print(f\"\\nğŸ‰ {len(exported_files)}å€‹ã®ãƒ¢ãƒ‡ãƒ«å½¢å¼ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ\")\n        for file_path in exported_files:\n            print(f\"  ğŸ“„ {file_path}\")\n    \n    return exported_files\n\n# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’å®Ÿè¡Œ\nif best_model:\n    exported_models = export_trained_model(best_model, config)\nelse:\n    print(\"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ - è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")\n    exported_models = []"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "copy-to-drive"
   },
   "outputs": [],
   "source": "# è¨“ç·´çµæœã‚’Google Driveã«ã‚³ãƒ”ãƒ¼\ndef copy_results_to_drive(config):\n    \"\"\"\n    è¨“ç·´çµæœã‚’Google Driveã«æ°¸ç¶šä¿å­˜ã™ã‚‹é–¢æ•°\n    \n    Args:\n        config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€å®Ÿé¨“åãªã©ã®æƒ…å ±ï¼‰\n    \n    Returns:\n        bool: ã‚³ãƒ”ãƒ¼ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹\n    \"\"\"\n    print(\"ğŸ’¾ è¨“ç·´çµæœã‚’Google Driveã«ã‚³ãƒ”ãƒ¼ä¸­...\")\n    \n    # ã‚³ãƒ”ãƒ¼å…ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆColabã®ä¸€æ™‚é ˜åŸŸï¼‰\n    source_dir = Path(config.project_name) / config.experiment_name\n    \n    # ã‚³ãƒ”ãƒ¼å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆGoogle Driveå†…ã®æ°¸ç¶šé ˜åŸŸï¼‰\n    drive_dir = Path('/content/drive/MyDrive/insect_detection_training/results') / config.experiment_name\n    \n    # ã‚³ãƒ”ãƒ¼å…ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª\n    if not source_dir.exists():\n        print(f\"âŒ ã‚³ãƒ”ãƒ¼å…ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {source_dir}\")\n        return False\n    \n    try:\n        # Google Driveå†…ã«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n        drive_dir.mkdir(parents=True, exist_ok=True)\n        \n        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’å†å¸°çš„ã«ã‚³ãƒ”ãƒ¼\n        # dirs_exist_ok=True: æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã£ã¦ã‚‚ä¸Šæ›¸ã\n        import shutil\n        shutil.copytree(source_dir, drive_dir, dirs_exist_ok=True)\n        \n        print(f\"âœ… çµæœã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {drive_dir}\")\n        \n        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªã¨è¡¨ç¤º\n        important_files = [\n            'weights/best.pt',      # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿\n            'weights/last.pt',      # æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®ãƒ¢ãƒ‡ãƒ«é‡ã¿\n            'results.png',          # è¨“ç·´çµæœã‚°ãƒ©ãƒ•\n            'confusion_matrix.png'  # æ··åŒè¡Œåˆ—\n        ]\n        \n        print(\"\\nğŸ“ Google Driveå†…ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«:\")\n        for file_path in important_files:\n            full_path = drive_dir / file_path\n            if full_path.exists():\n                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’MBå˜ä½ã§è¡¨ç¤º\n                size_mb = full_path.stat().st_size / (1024 * 1024)\n                print(f\"  âœ… {file_path} ({size_mb:.1f} MB)\")\n            else:\n                print(f\"  âŒ {file_path} (è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"âŒ Google Driveã¸ã®ã‚³ãƒ”ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}\")\n        return False\n\n# è¨“ç·´çµæœãŒå­˜åœ¨ã™ã‚‹å ´åˆã«Google Driveã¸ã‚³ãƒ”ãƒ¼\nif training_results:\n    copy_success = copy_results_to_drive(config)\nelse:\n    print(\"âš ï¸ Google Driveã¸ã®ã‚³ãƒ”ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ - è¨“ç·´çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-models"
   },
   "outputs": [],
   "source": [
    "# Download trained models to local computer\n",
    "def download_models(config):\n",
    "    print(\"â¬‡ï¸ Preparing model files for download...\")\n",
    "    \n",
    "    results_dir = Path(config.project_name) / config.experiment_name\n",
    "    weights_dir = results_dir / 'weights'\n",
    "    \n",
    "    if not weights_dir.exists():\n",
    "        print(f\"âŒ Weights directory not found: {weights_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Files to download\n",
    "    download_files = {\n",
    "        'best.pt': 'Best model weights',\n",
    "        'last.pt': 'Last epoch weights'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ“¥ Available for download:\")\n",
    "    \n",
    "    for filename, description in download_files.items():\n",
    "        file_path = weights_dir / filename\n",
    "        if file_path.exists():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ğŸ“„ {filename}: {description} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "            # Trigger download\n",
    "            try:\n",
    "                files.download(str(file_path))\n",
    "                print(f\"  âœ… {filename} download initiated\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error downloading {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"  âŒ {filename}: Not found\")\n",
    "    \n",
    "    # Also download results plot\n",
    "    results_plot = results_dir / 'results.png'\n",
    "    if results_plot.exists():\n",
    "        try:\n",
    "            files.download(str(results_plot))\n",
    "            print(f\"  âœ… results.png download initiated\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error downloading results.png: {e}\")\n",
    "\n",
    "# Download models\n",
    "if training_results:\n",
    "    download_models(config)\n",
    "else:\n",
    "    print(\"âš ï¸ No models available for download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": "## ğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—11: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-summary"
   },
   "outputs": [],
   "source": [
    "# Generate training summary\n",
    "def generate_training_summary(config, training_results, validation_results):\n",
    "    print(\"ğŸ“‹ TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"ğŸ¯ Project: {config.project_name}/{config.experiment_name}\")\n",
    "    print(f\"ğŸ¤– Model: {config.pretrained_model}\")\n",
    "    print(f\"ğŸ“Š Dataset: {config.data_yaml}\")\n",
    "    print(f\"âš™ï¸ Configuration:\")\n",
    "    print(f\"   - Epochs: {config.epochs}\")\n",
    "    print(f\"   - Batch Size: {config.batch_size}\")\n",
    "    print(f\"   - Image Size: {config.image_size}\")\n",
    "    print(f\"   - Device: {config.device}\")\n",
    "    \n",
    "    # Training status\n",
    "    if training_results:\n",
    "        print(f\"\\nâœ… Training Status: COMPLETED\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        if validation_results and hasattr(validation_results, 'box'):\n",
    "            box_metrics = validation_results.box\n",
    "            print(f\"\\nğŸ“Š Final Metrics:\")\n",
    "            print(f\"   - mAP@0.5: {box_metrics.map50:.4f}\")\n",
    "            print(f\"   - mAP@0.5:0.95: {box_metrics.map:.4f}\")\n",
    "            print(f\"   - Precision: {box_metrics.mp:.4f}\")\n",
    "            print(f\"   - Recall: {box_metrics.mr:.4f}\")\n",
    "            \n",
    "            # Performance evaluation\n",
    "            if box_metrics.map50 >= 0.7:\n",
    "                print(f\"   ğŸ‰ EXCELLENT: Model meets target performance (mAP@0.5 â‰¥ 0.7)\")\n",
    "            elif box_metrics.map50 >= 0.5:\n",
    "                print(f\"   âœ… GOOD: Model shows good performance (mAP@0.5 â‰¥ 0.5)\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ FAIR: Model needs improvement (mAP@0.5 < 0.5)\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Training Status: FAILED or INCOMPLETE\")\n",
    "    \n",
    "    # File locations\n",
    "    results_dir = Path(config.project_name) / config.experiment_name\n",
    "    drive_dir = Path('/content/drive/MyDrive/insect_detection_training/results') / config.experiment_name\n",
    "    \n",
    "    print(f\"\\nğŸ“ Output Locations:\")\n",
    "    print(f\"   - Local: {results_dir}\")\n",
    "    print(f\"   - Google Drive: {drive_dir}\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\nğŸš€ Next Steps:\")\n",
    "    print(f\"   1. Download model files (best.pt) for deployment\")\n",
    "    print(f\"   2. Test model on new images\")\n",
    "    print(f\"   3. Deploy to production environment\")\n",
    "    print(f\"   4. Monitor performance on real data\")\n",
    "    \n",
    "    if training_results:\n",
    "        print(f\"\\nğŸ’¡ Optimization Tips:\")\n",
    "        if validation_results and hasattr(validation_results, 'box'):\n",
    "            if validation_results.box.map50 < 0.7:\n",
    "                print(f\"   - Try training for more epochs\")\n",
    "                print(f\"   - Increase model size (yolov8s or yolov8m)\")\n",
    "                print(f\"   - Add more training data\")\n",
    "                print(f\"   - Adjust data augmentation\")\n",
    "            else:\n",
    "                print(f\"   - Model performance is good!\")\n",
    "                print(f\"   - Consider model compression for deployment\")\n",
    "                print(f\"   - Test on edge devices (Raspberry Pi)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Generate summary\n",
    "generate_training_summary(config, training_results, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usage-examples"
   },
   "outputs": [],
   "source": [
    "# Usage examples for trained model\n",
    "def show_usage_examples(config):\n",
    "    print(\"ğŸ’» USAGE EXAMPLES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model_path = f\"{config.project_name}/{config.experiment_name}/weights/best.pt\"\n",
    "    \n",
    "    print(\"\\nğŸ Python Usage:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from ultralytics import YOLO\")\n",
    "    print(\"\")\n",
    "    print(f\"# Load trained model\")\n",
    "    print(f\"model = YOLO('{model_path}')\")\n",
    "    print(\"\")\n",
    "    print(\"# Run inference on single image\")\n",
    "    print(\"results = model('path/to/image.jpg')\")\n",
    "    print(\"\")\n",
    "    print(\"# Run inference on multiple images\")\n",
    "    print(\"results = model(['img1.jpg', 'img2.jpg'])\")\n",
    "    print(\"\")\n",
    "    print(\"# Save results with annotations\")\n",
    "    print(\"for r in results:\")\n",
    "    print(\"    r.save(filename='result.jpg')\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\nğŸ–¥ï¸ Command Line Usage:\")\n",
    "    print(\"```bash\")\n",
    "    print(f\"# Single image prediction\")\n",
    "    print(f\"yolo predict model={model_path} source=image.jpg\")\n",
    "    print(\"\")\n",
    "    print(f\"# Batch prediction\")\n",
    "    print(f\"yolo predict model={model_path} source=images_folder/\")\n",
    "    print(\"\")\n",
    "    print(f\"# Video prediction\")\n",
    "    print(f\"yolo predict model={model_path} source=video.mp4\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\nğŸŒ Integration with detect_insect.py:\")\n",
    "    print(\"```bash\")\n",
    "    print(f\"# Use trained model with detection script\")\n",
    "    print(f\"python detect_insect.py \\\\\")\n",
    "    print(f\"    --input input_images/ \\\\\")\n",
    "    print(f\"    --output output_images/ \\\\\")\n",
    "    print(f\"    --model {model_path}\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\nğŸ“± Export for Edge Deployment:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Export to ONNX for cross-platform deployment\")\n",
    "    print(f\"model = YOLO('{model_path}')\")\n",
    "    print(\"model.export(format='onnx')\")\n",
    "    print(\"\")\n",
    "    print(\"# Export to TensorRT for NVIDIA GPUs\")\n",
    "    print(\"model.export(format='engine')\")\n",
    "    print(\"```\")\n",
    "\n",
    "# Show usage examples\n",
    "if training_results:\n",
    "    show_usage_examples(config)\n",
    "else:\n",
    "    print(\"âš ï¸ No usage examples available - training was not completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": "---\n\n## ğŸ‰ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼\n\n**ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼** æ˜†è™«æ¤œå‡ºã®ãŸã‚ã®YOLOv8ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚\n\n### ğŸ“‹ é”æˆã—ãŸå†…å®¹:\n- âœ… GPUåŠ é€Ÿãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n- âœ… ã‚«ãƒ–ãƒˆãƒ ã‚·æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨æ¤œè¨¼\n- âœ… ã‚«ã‚¹ã‚¿ãƒ YOLOv8ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n- âœ… ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©•ä¾¡\n- âœ… å±•é–‹ç”¨ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n- âœ… çµæœã®Google Driveã¸ã®ä¿å­˜\n\n### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n1. **è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰** (`best.pt`) ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½¿ç”¨\n2. **æ–°ã—ã„ã‚«ãƒ–ãƒˆãƒ ã‚·ç”»åƒã§ã®ãƒ†ã‚¹ãƒˆ**\n3. **æä¾›ã•ã‚ŒãŸä½¿ç”¨ä¾‹ã‚’ä½¿ç”¨ã—ãŸæœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹**\n4. **æ€§èƒ½ã®ç›£è¦–** ã¨å¿…è¦ã«å¿œã˜ãŸå†è¨“ç·´\n\n### ğŸ“š ãƒªã‚½ãƒ¼ã‚¹:\n- [YOLOv8 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.ultralytics.com/)\n- [ãƒ¢ãƒ‡ãƒ«å±•é–‹ã‚¬ã‚¤ãƒ‰](https://docs.ultralytics.com/modes/export/)\n- [æ€§èƒ½æœ€é©åŒ–](https://docs.ultralytics.com/guides/model-optimization/)\n\n---\n\n*ğŸ› æ¥½ã—ã„ã‚«ãƒ–ãƒˆãƒ ã‚·æ¤œå‡ºã‚’ï¼ ğŸ›*",
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}