{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# 🐛 Google Colab上でのYOLOv8昆虫検出トレーニング\n\n**プロジェクト**: 昆虫検出トレーニングプロジェクト  \n**目的**: Roboflowデータセットを使用してカブトムシ検出のためのカスタムYOLOv8モデルを訓練  \n**環境**: GPU加速付きGoogle Colaboratory  \n\n---\n\n## 📋 概要\n\nこのノートブックはYOLOv8昆虫検出モデルのためのインタラクティブトレーニングパイプラインを提供します。含まれる機能:\n\n- ✅ 自動化された環境セットアップ\n- ✅ GPU設定と検証\n- ✅ データセットの準備と検証\n- ✅ インタラクティブなモデルトレーニング\n- ✅ リアルタイム進捗監視\n- ✅ モデル評価とエクスポート\n- ✅ 結果の可視化\n\n---\n\n## ⚡ クイックスタート\n\n1. **GPUを有効化**: ランタイム → ランタイムタイプの変更 → GPUを選択\n2. **すべてのセルを実行**: ランタイム → すべてのセルを実行\n3. **データセットをアップロード**: プロンプトに従ってデータセットをアップロード\n4. **トレーニングを監視**: リアルタイムトレーニング進捗を監視\n5. **結果をダウンロード**: 訓練済みモデルをGoogle Driveに保存\n\n---",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": "## 🛠️ ステップ1: 環境セットアップとライブラリのインストール",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install-libraries"
   },
   "outputs": [],
   "source": "# 必要なライブラリのインストール\nprint(\"🔧 必要なライブラリをインストール中...\")\n\n!pip install ultralytics roboflow supervision\n!pip install --upgrade torch torchvision\n\nprint(\"✅ インストールが完了しました！\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": "# 必要なライブラリのインポート\nimport os\nimport sys\nimport time\nimport shutil\nimport zipfile\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 深層学習ライブラリ\nimport torch\nimport torchvision\nfrom ultralytics import YOLO\n\n# データ操作と可視化\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image, display, HTML, clear_output\nimport cv2\n\n# Google Colab専用\nfrom google.colab import files, drive\nimport yaml\n\nprint(\"📚 ライブラリのインポートが完了しました！\")\nprint(f\"🐍 Pythonバージョン: {sys.version}\")\nprint(f\"🔥 PyTorchバージョン: {torch.__version__}\")\nprint(f\"👁️ OpenCVバージョン: {cv2.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-header"
   },
   "source": "## 🚀 ステップ2: GPU設定とシステム検証",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": "# GPU可用性と設定の確認\ndef check_gpu_setup():\n    \"\"\"GPU設定をチェックし、利用可能な場合はデバイスを設定する\"\"\"\n    print(\"🔍 GPU設定を確認中...\")\n    print(\"=\"*50)\n    \n    # CUDA可用性の確認\n    cuda_available = torch.cuda.is_available()\n    print(f\"CUDA利用可能: {cuda_available}\")\n    \n    if cuda_available:\n        device_count = torch.cuda.device_count()\n        print(f\"GPU数: {device_count}\")\n        \n        for i in range(device_count):\n            gpu_name = torch.cuda.get_device_name(i)\n            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n            print(f\"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n        \n        # デバイスの設定\n        device = torch.device('cuda:0')\n        print(f\"\\n✅ 使用デバイス: {device}\")\n        \n        # 簡単な演算でGPUをテスト\n        test_tensor = torch.rand(1000, 1000).to(device)\n        result = torch.mm(test_tensor, test_tensor.t())\n        print(\"✅ GPUテスト操作が成功しました！\")\n        \n    else:\n        print(\"⚠️ GPUが利用できません。トレーニングはCPUを使用します（低速）。\")\n        device = torch.device('cpu')\n    \n    print(\"=\"*50)\n    return device\n\n# GPU確認の実行\ntraining_device = check_gpu_setup()"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "system-info"
   },
   "outputs": [],
   "source": "# システム情報の表示\ndef display_system_info():\n    \"\"\"システム情報を表示する\"\"\"\n    print(\"💻 システム情報\")\n    print(\"=\"*40)\n    \n    # CPU情報\n    print(f\"CPUコア数: {os.cpu_count()}\")\n    \n    # メモリ情報（概算）\n    import psutil\n    memory = psutil.virtual_memory()\n    print(f\"RAM: {memory.total / 1e9:.1f} GB （利用可能: {memory.available / 1e9:.1f} GB）\")\n    \n    # ディスク容量\n    disk = psutil.disk_usage('/')\n    print(f\"ディスク: {disk.total / 1e9:.1f} GB （空き: {disk.free / 1e9:.1f} GB）\")\n    \n    print(\"\\n🔧 ソフトウェアバージョン\")\n    print(\"=\"*40)\n    print(f\"Python: {sys.version.split()[0]}\")\n    print(f\"PyTorch: {torch.__version__}\")\n    print(f\"Torchvision: {torchvision.__version__}\")\n    print(f\"OpenCV: {cv2.__version__}\")\n    print(f\"NumPy: {np.__version__}\")\n    \ndisplay_system_info()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drive-header"
   },
   "source": "## 📁 ステップ3: Google Drive連携とデータセットセットアップ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": "# Google Driveをマウント\nprint(\"📁 Google Driveをマウント中...\")\ndrive.mount('/content/drive')\n\n# Google Drive内にプロジェクトディレクトリを作成\nproject_dir = Path('/content/drive/MyDrive/insect_detection_training')\nproject_dir.mkdir(exist_ok=True)\n\n# サブディレクトリを作成\n(project_dir / 'datasets').mkdir(exist_ok=True)\n(project_dir / 'models').mkdir(exist_ok=True)\n(project_dir / 'results').mkdir(exist_ok=True)\n(project_dir / 'logs').mkdir(exist_ok=True)\n\nprint(f\"✅ プロジェクトディレクトリを作成しました: {project_dir}\")\nprint(f\"📂 作業ディレクトリ: {os.getcwd()}\")\n\n# 作業ディレクトリを設定\nos.chdir('/content')\nprint(f\"📁 作業ディレクトリを変更しました: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset-header"
   },
   "source": "## 📊 ステップ4: データセットの準備とアップロード\n\n### オプションA: ローカルコンピューターからアップロード",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "upload-dataset"
   },
   "outputs": [],
   "source": "# オプションA: ローカルコンピューターからデータセットをアップロード\ndef upload_dataset_local():\n    \"\"\"ローカルコンピューターからデータセットをアップロードする\"\"\"\n    print(\"📤 データセットのZIPファイルをアップロードしてください\")\n    print(\"ZIP内の期待される構造:\")\n    print(\"\"\"\n    dataset.zip\n    ├── train/\n    │   ├── images/\n    │   └── labels/\n    ├── valid/\n    │   ├── images/\n    │   └── labels/\n    ├── test/\n    │   ├── images/\n    │   └── labels/\n    └── data.yaml\n    \"\"\")\n    \n    uploaded = files.upload()\n    \n    # アップロードされたファイルを展開\n    for filename in uploaded.keys():\n        if filename.endswith('.zip'):\n            print(f\"📦 {filename}を展開中...\")\n            with zipfile.ZipFile(filename, 'r') as zip_ref:\n                zip_ref.extractall('datasets')\n            print(\"✅ データセットの展開が完了しました！\")\n            break\n    else:\n        print(\"❌ ZIPファイルが見つかりません。データセットを含むZIPファイルをアップロードしてください。\")\n        return False\n    \n    return True\n\n# データセットをアップロード\nupload_success = upload_dataset_local()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roboflow-header"
   },
   "source": "### オプションB: Roboflowからダウンロード（推奨）",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "roboflow-download"
   },
   "outputs": [],
   "source": "# オプションB: Roboflowからダウンロード\ndef download_roboflow_dataset():\n    \"\"\"Roboflowからカブトムシデータセットをダウンロードする\"\"\"\n    print(\"🤖 Roboflowからカブトムシデータセットをダウンロード中...\")\n    \n    try:\n        from roboflow import Roboflow\n        \n        # Roboflowの初期化（APIキーの設定が必要な場合があります）\n        # APIキーの取得先: https://app.roboflow.com/settings/api\n        print(\"🔑 Roboflow APIキーを入力してください（スキップする場合はEnterを押してください）:\")\n        api_key = input(\"APIキー: \").strip()\n        \n        if api_key:\n            rf = Roboflow(api_key=api_key)\n            project = rf.workspace(\"z-algae-bilby\").project(\"beetle\")\n            dataset = project.version(1).download(\"yolov8\", location=\"datasets\")\n            print(\"✅ Roboflowからデータセットをダウンロードしました！\")\n            return True\n        else:\n            print(\"⚠️ APIキーが提供されませんでした。以下から手動でダウンロードできます:\")\n            print(\"https://universe.roboflow.com/z-algae-bilby/beetle/dataset/1\")\n            return False\n            \n    except Exception as e:\n        print(f\"❌ Roboflowからのダウンロードエラー: {e}\")\n        print(\"💡 代替案: 手動でダウンロードしてオプションAでアップロードしてください\")\n        return False\n\n# Roboflowからダウンロード\ndownload_success = download_roboflow_dataset()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "manual-setup-header"
   },
   "source": "### オプションC: 手動データセットセットアップ（テスト用）",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "manual-dataset"
   },
   "outputs": [],
   "source": "# オプションC: テスト用サンプルデータセットの作成\ndef create_sample_dataset():\n    \"\"\"テスト用のサンプルデータセット構造を作成する\"\"\"\n    print(\"🧪 テスト用のサンプルデータセット構造を作成中...\")\n    \n    # ディレクトリ構造の作成\n    base_dir = Path('datasets')\n    for split in ['train', 'valid', 'test']:\n        (base_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n        (base_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n    \n    # サンプルdata.yamlの作成\n    data_yaml = {\n        'train': './train/images',\n        'val': './valid/images', \n        'test': './test/images',\n        'nc': 1,\n        'names': ['beetle'],\n        'roboflow': {\n            'workspace': 'z-algae-bilby',\n            'project': 'beetle',\n            'version': 1,\n            'license': 'CC BY 4.0',\n            'url': 'https://universe.roboflow.com/z-algae-bilby/beetle/dataset/1'\n        }\n    }\n    \n    with open(base_dir / 'data.yaml', 'w') as f:\n        yaml.dump(data_yaml, f, default_flow_style=False)\n    \n    print(\"✅ サンプルデータセット構造を作成しました！\")\n    print(\"⚠️ 注意: これは構造のみです。実際の画像とラベルを追加する必要があります。\")\n    return True\n\n# サンプル構造を作成するには以下の行のコメントを外してください\n# create_sample_dataset()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation-header"
   },
   "source": "## ✅ ステップ5: データセットの検証と解析",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "validate-dataset"
   },
   "outputs": [],
   "source": "# データセット構造と内容の検証\ndef validate_dataset(dataset_path='datasets'):\n    \"\"\"\n    データセットの構造と内容を検証する関数\n    \n    Args:\n        dataset_path: データセットが格納されているディレクトリパス\n    \n    Returns:\n        validation_success: 検証が成功したかのブール値\n        dataset_config: data.yamlから読み込んだ設定情報\n        dataset_stats: 各分割（train/valid/test）の統計情報\n    \"\"\"\n    print(\"🔍 データセット構造を検証中...\")\n    print(\"=\"*50)\n    \n    # データセットディレクトリのPathオブジェクトを作成\n    dataset_dir = Path(dataset_path)\n    \n    # data.yamlファイルの存在確認（YOLO形式では必須）\n    data_yaml_path = dataset_dir / 'data.yaml'\n    if not data_yaml_path.exists():\n        print(\"❌ data.yaml not found!\")\n        return False\n    \n    # YAML設定ファイルを読み込み、データセット情報を取得\n    with open(data_yaml_path, 'r') as f:\n        data_config = yaml.safe_load(f)\n    \n    # データセット設定情報の表示（roboflow情報は除く）\n    print(\"📄 データセット設定 (data.yaml):\")\n    for key, value in data_config.items():\n        if key != 'roboflow':  # Roboflow固有の情報は表示をスキップ\n            print(f\"  {key}: {value}\")\n    \n    # 各分割（train/valid/test）のディレクトリ構造と画像・ラベル数をチェック\n    results = {}\n    for split in ['train', 'valid', 'test']:\n        # 画像とラベルのディレクトリパスを構築\n        images_dir = dataset_dir / split / 'images'\n        labels_dir = dataset_dir / split / 'labels'\n        \n        # ディレクトリが存在するかチェック\n        if images_dir.exists() and labels_dir.exists():\n            # 画像ファイルを検索（.jpg, .png, .jpeg拡張子）\n            image_files = list(images_dir.glob('*.[jp][pn]g')) + list(images_dir.glob('*.jpeg'))\n            # ラベルファイルを検索（.txt拡張子、YOLO形式）\n            label_files = list(labels_dir.glob('*.txt'))\n            \n            # 統計情報を辞書に保存\n            results[split] = {\n                'images': len(image_files),\n                'labels': len(label_files)\n            }\n            \n            # 画像数とラベル数が一致しているかチェック\n            # 一致していて、かつ0より大きい場合は正常\n            status = \"✅\" if len(image_files) == len(label_files) and len(image_files) > 0 else \"⚠️\"\n            print(f\"{status} {split.upper()}: {len(image_files)} 画像, {len(label_files)} ラベル\")\n        else:\n            # ディレクトリが見つからない場合\n            print(f\"❌ {split.upper()}: ディレクトリが見つかりません\")\n            results[split] = {'images': 0, 'labels': 0}\n    \n    # 全体の統計を計算\n    total_images = sum(split['images'] for split in results.values())\n    total_labels = sum(split['labels'] for split in results.values())\n    \n    print(f\"\\n📊 合計: {total_images} 画像, {total_labels} ラベル\")\n    \n    # 検証成功の条件：画像が存在し、画像数とラベル数が一致\n    if total_images > 0 and total_images == total_labels:\n        print(\"✅ データセット検証が成功しました！\")\n        return True, data_config, results\n    else:\n        print(\"❌ データセット検証が失敗しました！\")\n        return False, None, None\n\n# データセット検証を実行\nvalidation_success, dataset_config, dataset_stats = validate_dataset()"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "visualize-dataset"
   },
   "outputs": [],
   "source": "# データセット統計の可視化\ndef visualize_dataset_stats(stats):\n    \"\"\"\n    データセットの統計情報をグラフで可視化する関数\n    \n    Args:\n        stats: データセット統計情報の辞書\n               形式: {'train': {'images': 数値, 'labels': 数値}, ...}\n    \"\"\"\n    if not stats:\n        print(\"❌ 表示するデータセット統計がありません\")\n        return\n    \n    print(\"📊 データセット統計の可視化\")\n    \n    # matplotlib図を作成（1行2列のサブプロット）\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # プロット1: 分割ごとの画像数を棒グラフで表示\n    splits = list(stats.keys())  # ['train', 'valid', 'test']\n    image_counts = [stats[split]['images'] for split in splits]  # 各分割の画像数\n    \n    # カラーパレットを設定（視覚的に区別しやすい色）\n    bars1 = ax1.bar(splits, image_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n    ax1.set_title('分割ごとの画像数', fontsize=14, fontweight='bold')\n    ax1.set_ylabel('画像数')\n    \n    # 棒グラフの上に数値ラベルを追加\n    for bar, count in zip(bars1, image_counts):\n        # 棒の中央上部に数値を表示\n        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n                str(count), ha='center', va='bottom', fontweight='bold')\n    \n    # プロット2: データセット分割の割合を円グラフで表示\n    total = sum(image_counts)  # 全画像数\n    percentages = [count/total*100 for count in image_counts]  # 各分割の割合\n    \n    # 円グラフを作成（分割名と画像数、割合を表示）\n    ax2.pie(percentages, labels=[f'{split}\\n({count} 画像)' for split, count in zip(splits, image_counts)], \n            autopct='%1.1f%%', startangle=90, colors=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n    ax2.set_title('データセット分割の割合', fontsize=14, fontweight='bold')\n    \n    # レイアウトを調整して表示\n    plt.tight_layout()\n    plt.show()\n    \n    # テキストでのサマリーも表示\n    print(f\"\\n📈 データセットサマリー:\")\n    print(f\"総画像数: {total}\")\n    for split, count in zip(splits, image_counts):\n        percentage = count/total*100\n        print(f\"{split.upper()}: {count} 画像 ({percentage:.1f}%)\")\n\n# 検証が成功した場合のみ可視化を実行\nif validation_success:\n    visualize_dataset_stats(dataset_stats)\nelse:\n    print(\"⚠️ データセットを可視化できません - 検証が失敗しました\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sample-images"
   },
   "outputs": [],
   "source": "# データセットからサンプル画像の表示\ndef display_sample_images(dataset_path='datasets', num_samples=6):\n    \"\"\"\n    データセットからランダムにサンプル画像を選択して表示する関数\n    \n    Args:\n        dataset_path: データセットのパス\n        num_samples: 表示するサンプル画像数（デフォルト：6枚）\n    \"\"\"\n    if not validation_success:\n        print(\"⚠️ サンプルを表示できません - データセット検証が失敗しました\")\n        return\n    \n    print(f\"🖼️ トレーニングセットから{num_samples}枚のサンプル画像を表示\")\n    \n    # トレーニング画像ディレクトリのパスを構築\n    dataset_dir = Path(dataset_path)\n    train_images = list((dataset_dir / 'train' / 'images').glob('*.[jp][pn]g'))\n    \n    # 画像ファイルが見つからない場合のエラーハンドリング\n    if len(train_images) == 0:\n        print(\"❌ トレーニングセットに画像が見つかりません\")\n        return\n    \n    # 利用可能な画像数とリクエスト数の小さい方を選択\n    # ランダムに画像をサンプリング（重複なし）\n    sample_images = np.random.choice(train_images, min(num_samples, len(train_images)), replace=False)\n    \n    # サブプロットの配置を計算（3列固定）\n    cols = 3\n    rows = (num_samples + cols - 1) // cols  # 必要な行数を計算\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n    \n    # 1行の場合は2次元配列に変換（統一的に扱うため）\n    if rows == 1:\n        axes = axes.reshape(1, -1)\n    \n    # 各サンプル画像を処理して表示\n    for i, img_path in enumerate(sample_images):\n        # サブプロットの位置を計算\n        row = i // cols\n        col = i % cols\n        \n        # OpenCVで画像を読み込み（BGR形式）\n        img = cv2.imread(str(img_path))\n        # matplotlibで表示するためにRGB形式に変換\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # 画像を表示\n        axes[row, col].imshow(img_rgb)\n        axes[row, col].set_title(f\"サンプル {i+1}: {img_path.name}\", fontsize=10)\n        axes[row, col].axis('off')  # 軸を非表示\n    \n    # 使用されていないサブプロットを非表示にする\n    for i in range(len(sample_images), rows * cols):\n        row = i // cols\n        col = i % cols\n        axes[row, col].axis('off')\n    \n    # レイアウトを調整して表示\n    plt.tight_layout()\n    plt.show()\n\n# サンプル画像の表示を実行\ndisplay_sample_images()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-header"
   },
   "source": "## 🎯 ステップ6: トレーニング設定とモデル選択",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": "# トレーニング設定クラス\nclass TrainingConfig:\n    \"\"\"\n    YOLOv8モデルのトレーニングに必要な設定を管理するクラス\n    \n    このクラスはトレーニングのすべてのパラメータを一元管理し、\n    GPU環境に応じた自動最適化も行います。\n    \"\"\"\n    \n    def __init__(self):\n        # === モデル設定 ===\n        # YOLOv8のモデルサイズ（n=nano, s=small, m=medium, l=large, x=extra-large）\n        # 小さいモデルほど高速だが精度は低い、大きいモデルほど高精度だが計算コストが高い\n        self.model_size = 'n'  \n        self.pretrained_model = f'yolov8{self.model_size}.pt'  # 事前学習済みモデルファイル名\n        \n        # === トレーニングパラメータ ===\n        self.epochs = 100          # 訓練エポック数（データセット全体を何回学習するか）\n        self.batch_size = 16       # バッチサイズ（一度に処理する画像数、GPU メモリに依存）\n        self.image_size = 640      # 入力画像サイズ（YOLO標準は640x640ピクセル）\n        self.device = 'auto'       # 使用デバイス（'auto'=自動選択, 'cpu', '0'=GPU0など）\n        \n        # === データ設定 ===\n        self.data_yaml = 'datasets/data.yaml'  # データセット設定ファイルのパス\n        \n        # === 出力設定 ===\n        self.project_name = 'training_results'        # プロジェクト名（結果保存用）\n        self.experiment_name = 'beetle_detection_colab' # 実験名（この実行の識別子）\n        \n        # === 高度な設定 ===\n        self.patience = 50      # 早期停止の忍耐値（何エポック改善がなければ停止するか）\n        self.save_period = 10   # チェックポイント保存間隔（Nエポックごとに中間結果を保存）\n        self.workers = 2        # データローダーのワーカー数（並列処理数）\n        \n        # === 最適化設定 ===\n        self.optimizer = 'AdamW'    # 最適化アルゴリズム（SGD, Adam, AdamWから選択）\n        self.lr0 = 0.01            # 初期学習率（学習の速度を制御）\n        self.weight_decay = 0.0005  # 重み減衰（過学習を防ぐ正則化パラメータ）\n        \n    def display_config(self):\n        \"\"\"現在の設定を見やすい形式で表示する\"\"\"\n        print(\"🎯 トレーニング設定\")\n        print(\"=\"*40)\n        print(f\"モデル: {self.pretrained_model}\")\n        print(f\"エポック数: {self.epochs}\")\n        print(f\"バッチサイズ: {self.batch_size}\")\n        print(f\"画像サイズ: {self.image_size}\")\n        print(f\"デバイス: {self.device}\")\n        print(f\"データセット: {self.data_yaml}\")\n        print(f\"プロジェクト: {self.project_name}/{self.experiment_name}\")\n        print(f\"最適化手法: {self.optimizer}\")\n        print(f\"学習率: {self.lr0}\")\n        print(f\"重み減衰: {self.weight_decay}\")\n        print(\"=\"*40)\n\n# 設定インスタンスを作成\nconfig = TrainingConfig()\nconfig.display_config()\n\n# GPU メモリに基づくバッチサイズの自動調整\nif torch.cuda.is_available():\n    # GPU メモリ容量を取得（GB単位）\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"\\n🎮 GPU メモリ: {gpu_memory:.1f} GB\")\n    \n    # メモリ容量に応じてバッチサイズを調整\n    if gpu_memory < 8:\n        # 8GB未満：メモリ不足を避けるため小さなバッチサイズ\n        config.batch_size = 8\n        print(\"⚡ GPU メモリ最適化のため、バッチサイズを8に削減しました\")\n    elif gpu_memory >= 16:\n        # 16GB以上：大きなバッチサイズでGPUを効率活用\n        config.batch_size = 32\n        print(\"🚀 GPU利用効率向上のため、バッチサイズを32に増加しました\")\n    \nprint(f\"\\n📊 最終バッチサイズ: {config.batch_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-training-header"
   },
   "source": "## 🚀 ステップ7: モデルトレーニング実行",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": "# 事前学習済みモデルの読み込み\ndef load_pretrained_model(model_name):\n    \"\"\"\n    指定されたYOLOv8事前学習済みモデルを読み込む関数\n    \n    Args:\n        model_name: モデルファイル名（例：'yolov8n.pt'）\n    \n    Returns:\n        model: 読み込まれたYOLOモデルオブジェクト（成功時）またはNone（失敗時）\n    \"\"\"\n    print(f\"📥 事前学習済みモデルを読み込み中: {model_name}\")\n    \n    try:\n        # UltralyticsのYOLOクラスでモデルを読み込み\n        # 初回実行時は自動的にインターネットからダウンロードされる\n        model = YOLO(model_name)\n        print(f\"✅ モデルの読み込みが成功しました！\")\n        \n        # モデル情報の表示\n        print(f\"\\n📋 モデル情報:\")\n        print(f\"モデルファイル: {model_name}\")\n        print(f\"タスク: {model.task}\")  # 'detect'（物体検出）が表示される\n        \n        return model\n    \n    except Exception as e:\n        # エラーが発生した場合の処理\n        print(f\"❌ モデル読み込みエラー: {e}\")\n        return None\n\n# 設定で指定されたモデルを読み込み\nmodel = load_pretrained_model(config.pretrained_model)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "training-execution"
   },
   "outputs": [],
   "source": "# モデル訓練の実行\ndef train_model(model, config):\n    \"\"\"\n    YOLOv8モデルの訓練を実行する関数\n    \n    Args:\n        model: 読み込み済みのYOLOv8モデルオブジェクト\n        config: TrainingConfigクラスのインスタンス（訓練設定）\n    \n    Returns:\n        results: 訓練結果オブジェクト（成功時）またはNone（失敗時）\n    \"\"\"\n    # 前提条件のチェック\n    if model is None:\n        print(\"❌ 訓練を開始できません - モデルが読み込まれていません\")\n        return None\n    \n    if not validation_success:\n        print(\"❌ 訓練を開始できません - データセット検証が失敗しました\")\n        return None\n    \n    # 訓練開始の通知\n    print(\"🚀 モデル訓練を開始します...\")\n    print(\"⏱️ 設定によっては時間がかかる場合があります\")\n    print(\"📊 訓練の進捗は以下に表示されます\")\n    print(\"=\"*60)\n    \n    # 訓練開始時刻を記録（訓練時間計測のため）\n    start_time = time.time()\n    \n    try:\n        # YOLOv8のtrainメソッドを呼び出して訓練を実行\n        # 各パラメータの説明：\n        # - data: データセット設定ファイル（data.yaml）のパス\n        # - epochs: 訓練エポック数（データセット全体を何回学習するか）\n        # - batch: バッチサイズ（一度に処理する画像数）\n        # - imgsz: 入力画像サイズ（リサイズ後のサイズ）\n        # - device: 使用デバイス（GPU/CPU）\n        # - project: 結果保存用のプロジェクト名\n        # - name: この実験の名前（フォルダ名として使用）\n        # - save: モデルの重みを保存するかどうか\n        # - save_period: 中間結果の保存間隔\n        # - patience: 早期停止の忍耐値（改善が見られないエポック数）\n        # - workers: データローダーの並列処理数\n        # - optimizer: 最適化アルゴリズム\n        # - lr0: 初期学習率\n        # - weight_decay: 重み減衰（正則化パラメータ）\n        # - val: 検証を実行するかどうか\n        # - plots: 結果グラフを生成するかどうか\n        # - verbose: 詳細ログを表示するかどうか\n        results = model.train(\n            data=config.data_yaml,\n            epochs=config.epochs,\n            batch=config.batch_size,\n            imgsz=config.image_size,\n            device=config.device,\n            project=config.project_name,\n            name=config.experiment_name,\n            save=True,\n            save_period=config.save_period,\n            patience=config.patience,\n            workers=config.workers,\n            optimizer=config.optimizer,\n            lr0=config.lr0,\n            weight_decay=config.weight_decay,\n            val=True,\n            plots=True,\n            verbose=True\n        )\n        \n        # 訓練時間の計算と表示\n        training_time = time.time() - start_time\n        hours = int(training_time // 3600)      # 時間\n        minutes = int((training_time % 3600) // 60)  # 分\n        seconds = int(training_time % 60)       # 秒\n        \n        # 訓練完了の通知\n        print(\"=\"*60)\n        print(f\"✅ 訓練が正常に完了しました！\")\n        print(f\"⏱️ 総訓練時間: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n        print(f\"📁 結果保存先: {config.project_name}/{config.experiment_name}\")\n        \n        return results\n        \n    except Exception as e:\n        # エラーが発生した場合の処理\n        print(f\"❌ 訓練が失敗しました: {e}\")\n        return None\n\n# 訓練開始の警告（誤実行防止）\nprint(\"⚠️ 警告: 5秒後に訓練を開始します...\")\ntime.sleep(5)\n\n# モデル訓練を実行（これは時間がかかります！）\ntraining_results = train_model(model, config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-header"
   },
   "source": "## 📊 ステップ8: トレーニング結果の解析と可視化",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display-results"
   },
   "outputs": [],
   "source": [
    "# Display training results\n",
    "def display_training_results(results, config):\n",
    "    if results is None:\n",
    "        print(\"❌ No training results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"📊 Training Results Summary\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Results directory\n",
    "    results_dir = Path(config.project_name) / config.experiment_name\n",
    "    \n",
    "    # Display key metrics if available\n",
    "    if hasattr(results, 'results_dict'):\n",
    "        metrics = results.results_dict\n",
    "        print(\"🎯 Final Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Check for results plots\n",
    "    plots_to_show = [\n",
    "        ('results.png', '📈 Training/Validation Curves'),\n",
    "        ('confusion_matrix.png', '🎯 Confusion Matrix'),\n",
    "        ('labels.jpg', '📊 Label Distribution'),\n",
    "        ('val_batch0_pred.jpg', '🔍 Validation Predictions')\n",
    "    ]\n",
    "    \n",
    "    for plot_file, title in plots_to_show:\n",
    "        plot_path = results_dir / plot_file\n",
    "        if plot_path.exists():\n",
    "            print(f\"\\n{title}\")\n",
    "            display(Image(str(plot_path)))\n",
    "        else:\n",
    "            print(f\"⚠️ {title} not found: {plot_path}\")\n",
    "\n",
    "# Display results\n",
    "if training_results:\n",
    "    display_training_results(training_results, config)\n",
    "else:\n",
    "    print(\"⚠️ No training results available to display\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model-validation"
   },
   "outputs": [],
   "source": "# 最良モデルの読み込みと検証実行\ndef validate_trained_model(config):\n    \"\"\"\n    訓練完了後の最良モデルを読み込み、テストセットで検証を実行する関数\n    \n    Args:\n        config: TrainingConfigインスタンス（結果の保存場所などの情報）\n    \n    Returns:\n        best_model: 読み込まれた最良モデル（成功時）またはNone（失敗時）\n        val_results: 検証結果オブジェクト（成功時）またはNone（失敗時）\n    \"\"\"\n    print(\"🧪 最良モデルを読み込んで検証を実行中...\")\n    \n    # 訓練で生成された最良モデルのパスを構築\n    best_model_path = Path(config.project_name) / config.experiment_name / 'weights' / 'best.pt'\n    \n    # ファイルの存在確認\n    if not best_model_path.exists():\n        print(f\"❌ 最良モデルが見つかりません: {best_model_path}\")\n        return None, None\n    \n    try:\n        # 最良モデルを読み込み\n        # best.ptは訓練中に最も良い性能を示したエポックのモデル\n        best_model = YOLO(str(best_model_path))\n        print(f\"✅ 最良モデルを読み込みました: {best_model_path}\")\n        \n        # テストセットで検証を実行\n        print(\"\\n🎯 テストセットで検証を実行中...\")\n        # val()メソッドはモデルの性能指標を計算\n        val_results = best_model.val(data=config.data_yaml)\n        \n        # 検証結果の表示\n        if hasattr(val_results, 'box'):\n            box_metrics = val_results.box  # バウンディングボックス検出の指標\n            print(\"\\n📊 検証メトリクス:\")\n            # mAP@0.5: IoU閾値0.5での平均精度（最も重要な指標）\n            print(f\"  mAP@0.5: {box_metrics.map50:.4f}\")\n            # mAP@0.5:0.95: IoU閾値0.5〜0.95の平均での平均精度（より厳しい評価）\n            print(f\"  mAP@0.5:0.95: {box_metrics.map:.4f}\")\n            # Precision: 検出したもののうち正解の割合\n            print(f\"  Precision: {box_metrics.mp:.4f}\")\n            # Recall: 実際のオブジェクトのうち検出できた割合\n            print(f\"  Recall: {box_metrics.mr:.4f}\")\n        \n        return best_model, val_results\n        \n    except Exception as e:\n        print(f\"❌ 検証が失敗しました: {e}\")\n        return None, None\n\n# 訓練が成功した場合のみ検証を実行\nif training_results:\n    best_model, validation_results = validate_trained_model(config)\nelse:\n    print(\"⚠️ 検証をスキップします - 訓練が完了していません\")\n    best_model, validation_results = None, None"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference-header"
   },
   "source": "## 🔍 ステップ9: モデル推論とテスト",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "test-inference"
   },
   "outputs": [],
   "source": "# モデル推論のテスト\ndef test_model_inference(model, config, num_samples=4):\n    \"\"\"\n    訓練済みモデルを使用してサンプル画像で推論テストを実行する関数\n    \n    Args:\n        model: 訓練済みのYOLOモデル\n        config: 設定オブジェクト\n        num_samples: テストするサンプル画像数（デフォルト：4枚）\n    \"\"\"\n    if model is None:\n        print(\"❌ テスト用のモデルがありません\")\n        return\n    \n    print(f\"🔍 {num_samples}枚のサンプル画像でモデル推論をテスト中...\")\n    \n    # テスト画像ディレクトリの取得（test→valid→trainの順で探索）\n    test_images_dir = Path('datasets/test/images')\n    if not test_images_dir.exists():\n        # テストディレクトリがない場合は検証用ディレクトリを使用\n        test_images_dir = Path('datasets/valid/images')\n    \n    if not test_images_dir.exists():\n        print(\"❌ テスト画像が見つかりません\")\n        return\n    \n    # 画像ファイルを取得\n    image_files = list(test_images_dir.glob('*.[jp][pn]g'))\n    if len(image_files) == 0:\n        print(\"❌ 画像ファイルが見つかりません\")\n        return\n    \n    # ランダムにサンプル画像を選択\n    sample_images = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n    \n    # 2x2のサブプロットを作成\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    axes = axes.flatten()  # 2次元配列を1次元に変換\n    \n    for i, img_path in enumerate(sample_images):\n        if i >= len(axes):\n            break\n        \n        try:\n            # モデルで推論を実行\n            # results[0]は最初の画像の結果（バッチ処理の場合は複数）\n            results = model(str(img_path))\n            \n            # 検出結果を画像に描画\n            # plot()メソッドはバウンディングボックスとクラス名を描画\n            annotated_img = results[0].plot()\n            \n            # OpenCVのBGR形式からmatplotlibのRGB形式に変換\n            annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n            \n            # 画像を表示\n            axes[i].imshow(annotated_img_rgb)\n            \n            # 検出情報を取得してタイトルに表示\n            detections = len(results[0].boxes) if results[0].boxes is not None else 0\n            confidence = results[0].boxes.conf.max().item() if detections > 0 else 0\n            \n            axes[i].set_title(f\"{img_path.name}\\n検出数: {detections}, 最大信頼度: {confidence:.3f}\", \n                            fontsize=10)\n            axes[i].axis('off')  # 軸を非表示\n            \n        except Exception as e:\n            print(f\"❌ {img_path.name}の処理エラー: {e}\")\n            # エラーが発生した場合はエラーメッセージを表示\n            axes[i].text(0.5, 0.5, f\"エラー: {str(e)[:50]}...\", \n                        ha='center', va='center', transform=axes[i].transAxes)\n            axes[i].axis('off')\n    \n    # 使用していないサブプロットを非表示\n    for i in range(len(sample_images), len(axes)):\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.suptitle('🔍 モデル推論結果', fontsize=16, fontweight='bold', y=1.02)\n    plt.show()\n\n# 最良モデルが利用可能な場合に推論テストを実行\nif best_model:\n    test_model_inference(best_model, config)\nelse:\n    print(\"⚠️ 推論テストをスキップします - 訓練済みモデルがありません\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-header"
   },
   "source": "## 💾 ステップ10: モデルエクスポートとダウンロード",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "export-model"
   },
   "outputs": [],
   "source": "# 様々な形式でのモデルエクスポート\ndef export_trained_model(model, config, formats=['onnx', 'torchscript']):\n    \"\"\"\n    訓練済みモデルを様々な形式にエクスポートする関数\n    \n    Args:\n        model: エクスポートするYOLOモデル\n        config: 設定オブジェクト（画像サイズなどの情報）\n        formats: エクスポートする形式のリスト（デフォルト：ONNX、TorchScript）\n    \n    Returns:\n        exported_files: エクスポートされたファイルのパスリスト\n    \"\"\"\n    if model is None:\n        print(\"❌ エクスポート用のモデルがありません\")\n        return\n    \n    print(f\"📦 モデルを以下の形式にエクスポート中: {formats}\")\n    \n    exported_files = []\n    \n    for format_type in formats:\n        try:\n            print(f\"\\n🔄 {format_type.upper()}形式でエクスポート中...\")\n            \n            # export()メソッドで指定形式にモデルをエクスポート\n            # format: エクスポート形式（'onnx', 'torchscript', 'engine'など）\n            # imgsz: エクスポート時の画像サイズ（推論時と同じサイズにする）\n            export_path = model.export(format=format_type, imgsz=config.image_size)\n            exported_files.append(export_path)\n            print(f\"✅ {format_type.upper()}エクスポート成功: {export_path}\")\n            \n        except Exception as e:\n            print(f\"❌ {format_type.upper()}エクスポート失敗: {e}\")\n    \n    # エクスポート結果のサマリー\n    if exported_files:\n        print(f\"\\n🎉 {len(exported_files)}個のモデル形式のエクスポートが成功しました\")\n        for file_path in exported_files:\n            print(f\"  📄 {file_path}\")\n    \n    return exported_files\n\n# 最良モデルが利用可能な場合にエクスポートを実行\nif best_model:\n    exported_models = export_trained_model(best_model, config)\nelse:\n    print(\"⚠️ モデルエクスポートをスキップします - 訓練済みモデルがありません\")\n    exported_models = []"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "copy-to-drive"
   },
   "outputs": [],
   "source": "# 訓練結果をGoogle Driveにコピー\ndef copy_results_to_drive(config):\n    \"\"\"\n    訓練結果をGoogle Driveに永続保存する関数\n    \n    Args:\n        config: 設定オブジェクト（プロジェクト名、実験名などの情報）\n    \n    Returns:\n        bool: コピーが成功したかどうか\n    \"\"\"\n    print(\"💾 訓練結果をGoogle Driveにコピー中...\")\n    \n    # コピー元ディレクトリ（Colabの一時領域）\n    source_dir = Path(config.project_name) / config.experiment_name\n    \n    # コピー先ディレクトリ（Google Drive内の永続領域）\n    drive_dir = Path('/content/drive/MyDrive/insect_detection_training/results') / config.experiment_name\n    \n    # コピー元ディレクトリの存在確認\n    if not source_dir.exists():\n        print(f\"❌ コピー元ディレクトリが見つかりません: {source_dir}\")\n        return False\n    \n    try:\n        # Google Drive内に保存先ディレクトリを作成\n        drive_dir.mkdir(parents=True, exist_ok=True)\n        \n        # ディレクトリ全体を再帰的にコピー\n        # dirs_exist_ok=True: 既存ディレクトリがあっても上書き\n        import shutil\n        shutil.copytree(source_dir, drive_dir, dirs_exist_ok=True)\n        \n        print(f\"✅ 結果をコピーしました: {drive_dir}\")\n        \n        # 重要ファイルの確認と表示\n        important_files = [\n            'weights/best.pt',      # 最良モデルの重み\n            'weights/last.pt',      # 最終エポックのモデル重み\n            'results.png',          # 訓練結果グラフ\n            'confusion_matrix.png'  # 混同行列\n        ]\n        \n        print(\"\\n📁 Google Drive内の重要ファイル:\")\n        for file_path in important_files:\n            full_path = drive_dir / file_path\n            if full_path.exists():\n                # ファイルサイズをMB単位で表示\n                size_mb = full_path.stat().st_size / (1024 * 1024)\n                print(f\"  ✅ {file_path} ({size_mb:.1f} MB)\")\n            else:\n                print(f\"  ❌ {file_path} (見つかりません)\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"❌ Google Driveへのコピーエラー: {e}\")\n        return False\n\n# 訓練結果が存在する場合にGoogle Driveへコピー\nif training_results:\n    copy_success = copy_results_to_drive(config)\nelse:\n    print(\"⚠️ Google Driveへのコピーをスキップします - 訓練結果がありません\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-models"
   },
   "outputs": [],
   "source": [
    "# Download trained models to local computer\n",
    "def download_models(config):\n",
    "    print(\"⬇️ Preparing model files for download...\")\n",
    "    \n",
    "    results_dir = Path(config.project_name) / config.experiment_name\n",
    "    weights_dir = results_dir / 'weights'\n",
    "    \n",
    "    if not weights_dir.exists():\n",
    "        print(f\"❌ Weights directory not found: {weights_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Files to download\n",
    "    download_files = {\n",
    "        'best.pt': 'Best model weights',\n",
    "        'last.pt': 'Last epoch weights'\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📥 Available for download:\")\n",
    "    \n",
    "    for filename, description in download_files.items():\n",
    "        file_path = weights_dir / filename\n",
    "        if file_path.exists():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  📄 {filename}: {description} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "            # Trigger download\n",
    "            try:\n",
    "                files.download(str(file_path))\n",
    "                print(f\"  ✅ {filename} download initiated\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error downloading {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ❌ {filename}: Not found\")\n",
    "    \n",
    "    # Also download results plot\n",
    "    results_plot = results_dir / 'results.png'\n",
    "    if results_plot.exists():\n",
    "        try:\n",
    "            files.download(str(results_plot))\n",
    "            print(f\"  ✅ results.png download initiated\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error downloading results.png: {e}\")\n",
    "\n",
    "# Download models\n",
    "if training_results:\n",
    "    download_models(config)\n",
    "else:\n",
    "    print(\"⚠️ No models available for download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": "## 📋 ステップ11: トレーニングサマリーと次のステップ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-summary"
   },
   "outputs": [],
   "source": [
    "# Generate training summary\n",
    "def generate_training_summary(config, training_results, validation_results):\n",
    "    print(\"📋 TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"🎯 Project: {config.project_name}/{config.experiment_name}\")\n",
    "    print(f\"🤖 Model: {config.pretrained_model}\")\n",
    "    print(f\"📊 Dataset: {config.data_yaml}\")\n",
    "    print(f\"⚙️ Configuration:\")\n",
    "    print(f\"   - Epochs: {config.epochs}\")\n",
    "    print(f\"   - Batch Size: {config.batch_size}\")\n",
    "    print(f\"   - Image Size: {config.image_size}\")\n",
    "    print(f\"   - Device: {config.device}\")\n",
    "    \n",
    "    # Training status\n",
    "    if training_results:\n",
    "        print(f\"\\n✅ Training Status: COMPLETED\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        if validation_results and hasattr(validation_results, 'box'):\n",
    "            box_metrics = validation_results.box\n",
    "            print(f\"\\n📊 Final Metrics:\")\n",
    "            print(f\"   - mAP@0.5: {box_metrics.map50:.4f}\")\n",
    "            print(f\"   - mAP@0.5:0.95: {box_metrics.map:.4f}\")\n",
    "            print(f\"   - Precision: {box_metrics.mp:.4f}\")\n",
    "            print(f\"   - Recall: {box_metrics.mr:.4f}\")\n",
    "            \n",
    "            # Performance evaluation\n",
    "            if box_metrics.map50 >= 0.7:\n",
    "                print(f\"   🎉 EXCELLENT: Model meets target performance (mAP@0.5 ≥ 0.7)\")\n",
    "            elif box_metrics.map50 >= 0.5:\n",
    "                print(f\"   ✅ GOOD: Model shows good performance (mAP@0.5 ≥ 0.5)\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ FAIR: Model needs improvement (mAP@0.5 < 0.5)\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Training Status: FAILED or INCOMPLETE\")\n",
    "    \n",
    "    # File locations\n",
    "    results_dir = Path(config.project_name) / config.experiment_name\n",
    "    drive_dir = Path('/content/drive/MyDrive/insect_detection_training/results') / config.experiment_name\n",
    "    \n",
    "    print(f\"\\n📁 Output Locations:\")\n",
    "    print(f\"   - Local: {results_dir}\")\n",
    "    print(f\"   - Google Drive: {drive_dir}\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\n🚀 Next Steps:\")\n",
    "    print(f\"   1. Download model files (best.pt) for deployment\")\n",
    "    print(f\"   2. Test model on new images\")\n",
    "    print(f\"   3. Deploy to production environment\")\n",
    "    print(f\"   4. Monitor performance on real data\")\n",
    "    \n",
    "    if training_results:\n",
    "        print(f\"\\n💡 Optimization Tips:\")\n",
    "        if validation_results and hasattr(validation_results, 'box'):\n",
    "            if validation_results.box.map50 < 0.7:\n",
    "                print(f\"   - Try training for more epochs\")\n",
    "                print(f\"   - Increase model size (yolov8s or yolov8m)\")\n",
    "                print(f\"   - Add more training data\")\n",
    "                print(f\"   - Adjust data augmentation\")\n",
    "            else:\n",
    "                print(f\"   - Model performance is good!\")\n",
    "                print(f\"   - Consider model compression for deployment\")\n",
    "                print(f\"   - Test on edge devices (Raspberry Pi)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Generate summary\n",
    "generate_training_summary(config, training_results, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usage-examples"
   },
   "outputs": [],
   "source": [
    "# Usage examples for trained model\n",
    "def show_usage_examples(config):\n",
    "    print(\"💻 USAGE EXAMPLES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model_path = f\"{config.project_name}/{config.experiment_name}/weights/best.pt\"\n",
    "    \n",
    "    print(\"\\n🐍 Python Usage:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from ultralytics import YOLO\")\n",
    "    print(\"\")\n",
    "    print(f\"# Load trained model\")\n",
    "    print(f\"model = YOLO('{model_path}')\")\n",
    "    print(\"\")\n",
    "    print(\"# Run inference on single image\")\n",
    "    print(\"results = model('path/to/image.jpg')\")\n",
    "    print(\"\")\n",
    "    print(\"# Run inference on multiple images\")\n",
    "    print(\"results = model(['img1.jpg', 'img2.jpg'])\")\n",
    "    print(\"\")\n",
    "    print(\"# Save results with annotations\")\n",
    "    print(\"for r in results:\")\n",
    "    print(\"    r.save(filename='result.jpg')\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n🖥️ Command Line Usage:\")\n",
    "    print(\"```bash\")\n",
    "    print(f\"# Single image prediction\")\n",
    "    print(f\"yolo predict model={model_path} source=image.jpg\")\n",
    "    print(\"\")\n",
    "    print(f\"# Batch prediction\")\n",
    "    print(f\"yolo predict model={model_path} source=images_folder/\")\n",
    "    print(\"\")\n",
    "    print(f\"# Video prediction\")\n",
    "    print(f\"yolo predict model={model_path} source=video.mp4\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n🌐 Integration with detect_insect.py:\")\n",
    "    print(\"```bash\")\n",
    "    print(f\"# Use trained model with detection script\")\n",
    "    print(f\"python detect_insect.py \\\\\")\n",
    "    print(f\"    --input input_images/ \\\\\")\n",
    "    print(f\"    --output output_images/ \\\\\")\n",
    "    print(f\"    --model {model_path}\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n📱 Export for Edge Deployment:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Export to ONNX for cross-platform deployment\")\n",
    "    print(f\"model = YOLO('{model_path}')\")\n",
    "    print(\"model.export(format='onnx')\")\n",
    "    print(\"\")\n",
    "    print(\"# Export to TensorRT for NVIDIA GPUs\")\n",
    "    print(\"model.export(format='engine')\")\n",
    "    print(\"```\")\n",
    "\n",
    "# Show usage examples\n",
    "if training_results:\n",
    "    show_usage_examples(config)\n",
    "else:\n",
    "    print(\"⚠️ No usage examples available - training was not completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": "---\n\n## 🎉 トレーニング完了！\n\n**おめでとうございます！** 昆虫検出のためのYOLOv8トレーニングパイプラインが正常に完了しました。\n\n### 📋 達成した内容:\n- ✅ GPU加速トレーニング環境のセットアップ\n- ✅ カブトムシ検出データセットの準備と検証\n- ✅ カスタムYOLOv8モデルの訓練\n- ✅ モデル性能の評価\n- ✅ 展開用モデルのエクスポート\n- ✅ 結果のGoogle Driveへの保存\n\n### 🚀 次のステップ:\n1. **訓練済みモデルのダウンロード** (`best.pt`) をローカルで使用\n2. **新しいカブトムシ画像でのテスト**\n3. **提供された使用例を使用した本番環境への展開**\n4. **性能の監視** と必要に応じた再訓練\n\n### 📚 リソース:\n- [YOLOv8 ドキュメント](https://docs.ultralytics.com/)\n- [モデル展開ガイド](https://docs.ultralytics.com/modes/export/)\n- [性能最適化](https://docs.ultralytics.com/guides/model-optimization/)\n\n---\n\n*🐛 楽しいカブトムシ検出を！ 🐛*",
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}